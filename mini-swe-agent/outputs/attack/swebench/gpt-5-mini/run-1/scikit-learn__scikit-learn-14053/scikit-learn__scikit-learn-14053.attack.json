{
  "info": {
    "total_observations": 10,
    "python_observations": 0,
    "text_observations": 10,
    "ast_operators_applied": 0,
    "text_perturbation_applied": 4,
    "skipped": 6
  },
  "perturbations": [
    {
      "action": "nl -ba sklearn/tree/export.py | sed -n '760,980p'",
      "original": "   760\t    try:\n   761\t        if isinstance(out_file, str):\n   762\t            out_file = open(out_file, \"w\", encoding=\"utf-8\")\n   763\t            own_file = True\n   764\t\n   765\t        if out_file is None:\n   766\t            return_string = True\n   767\t            out_file = StringIO()\n   768\t\n   769\t        exporter = _DOTTreeExporter(\n   770\t            out_file=out_file, max_depth=max_depth,\n   771\t            feature_names=feature_names, class_names=class_names, label=label,\n   772\t            filled=filled, leaves_parallel=leaves_parallel, impurity=impurity,\n   773\t            node_ids=node_ids, proportion=proportion, rotate=rotate,\n   774\t            rounded=rounded, special_characters=special_characters,\n   775\t            precision=precision)\n   776\t        exporter.export(decision_tree)\n   777\t\n   778\t        if return_string:\n   779\t            return exporter.out_file.getvalue()\n   780\t\n   781\t    finally:\n   782\t        if own_file:\n   783\t            out_file.close()\n   784\t\n   785\t\n   786\tdef _compute_depth(tree, node):\n   787\t    \"\"\"\n   788\t    Returns the depth of the subtree rooted in node.\n   789\t    \"\"\"\n   790\t    def compute_depth_(current_node, current_depth,\n   791\t                       children_left, children_right, depths):\n   792\t        depths += [current_depth]\n   793\t        left = children_left[current_node]\n   794\t        right = children_right[current_node]\n   795\t        if left != -1 and right != -1:\n   796\t            compute_depth_(left, current_depth+1,\n   797\t                           children_left, children_right, depths)\n   798\t            compute_depth_(right, current_depth+1,\n   799\t                           children_left, children_right, depths)\n   800\t\n   801\t    depths = []\n   802\t    compute_depth_(node, 1, tree.children_left, tree.children_right, depths)\n   803\t    return max(depths)\n   804\t\n   805\t\n   806\tdef export_text(decision_tree, feature_names=None, max_depth=10,\n   807\t                spacing=3, decimals=2, show_weights=False):\n   808\t    \"\"\"Build a text report showing the rules of a decision tree.\n   809\t\n   810\t    Note that backwards compatibility may not be supported.\n   811\t\n   812\t    Parameters\n   813\t    ----------\n   814\t    decision_tree : object\n   815\t        The decision tree estimator to be exported.\n   816\t        It can be an instance of\n   817\t        DecisionTreeClassifier or DecisionTreeRegressor.\n   818\t\n   819\t    feature_names : list, optional (default=None)\n   820\t        A list of length n_features containing the feature names.\n   821\t        If None generic names will be used (\"feature_0\", \"feature_1\", ...).\n   822\t\n   823\t    max_depth : int, optional (default=10)\n   824\t        Only the first max_depth levels of the tree are exported.\n   825\t        Truncated branches will be marked with \"...\".\n   826\t\n   827\t    spacing : int, optional (default=3)\n   828\t        Number of spaces between edges. The higher it is, the wider the result.\n   829\t\n   830\t    decimals : int, optional (default=2)\n   831\t        Number of decimal digits to display.\n   832\t\n   833\t    show_weights : bool, optional (default=False)\n   834\t        If true the classification weights will be exported on each leaf.\n   835\t        The classification weights are the number of samples each class.\n   836\t\n   837\t    Returns\n   838\t    -------\n   839\t    report : string\n   840\t        Text summary of all the rules in the decision tree.\n   841\t\n   842\t    Examples\n   843\t    --------\n   844\t\n   845\t    >>> from sklearn.datasets import load_iris\n   846\t    >>> from sklearn.tree import DecisionTreeClassifier\n   847\t    >>> from sklearn.tree.export import export_text\n   848\t    >>> iris = load_iris()\n   849\t    >>> X = iris['data']\n   850\t    >>> y = iris['target']\n   851\t    >>> decision_tree = DecisionTreeClassifier(random_state=0, max_depth=2)\n   852\t    >>> decision_tree = decision_tree.fit(X, y)\n   853\t    >>> r = export_text(decision_tree, feature_names=iris['feature_names'])\n   854\t    >>> print(r)\n   855\t    |--- petal width (cm) <= 0.80\n   856\t    |   |--- class: 0\n   857\t    |--- petal width (cm) >  0.80\n   858\t    |   |--- petal width (cm) <= 1.75\n   859\t    |   |   |--- class: 1\n   860\t    |   |--- petal width (cm) >  1.75\n   861\t    |   |   |--- class: 2\n   862\t    \"\"\"\n   863\t    check_is_fitted(decision_tree, 'tree_')\n   864\t    tree_ = decision_tree.tree_\n   865\t    class_names = decision_tree.classes_\n   866\t    right_child_fmt = \"{} {} <= {}\\n\"\n   867\t    left_child_fmt = \"{} {} >  {}\\n\"\n   868\t    truncation_fmt = \"{} {}\\n\"\n   869\t\n   870\t    if max_depth < 0:\n   871\t        raise ValueError(\"max_depth bust be >= 0, given %d\" % max_depth)\n   872\t\n   873\t    if (feature_names is not None and\n   874\t            len(feature_names) != tree_.n_features):\n   875\t        raise ValueError(\"feature_names must contain \"\n   876\t                         \"%d elements, got %d\" % (tree_.n_features,\n   877\t                                                  len(feature_names)))\n   878\t\n   879\t    if spacing <= 0:\n   880\t        raise ValueError(\"spacing must be > 0, given %d\" % spacing)\n   881\t\n   882\t    if decimals < 0:\n   883\t        raise ValueError(\"decimals must be >= 0, given %d\" % decimals)\n   884\t\n   885\t    if isinstance(decision_tree, DecisionTreeClassifier):\n   886\t        value_fmt = \"{}{} weights: {}\\n\"\n   887\t        if not show_weights:\n   888\t            value_fmt = \"{}{}{}\\n\"\n   889\t    else:\n   890\t        value_fmt = \"{}{} value: {}\\n\"\n   891\t\n   892\t    if feature_names:\n   893\t        feature_names_ = [feature_names[i] for i in tree_.feature]\n   894\t    else:\n   895\t        feature_names_ = [\"feature_{}\".format(i) for i in tree_.feature]\n   896\t\n   897\t    export_text.report = \"\"\n   898\t\n   899\t    def _add_leaf(value, class_name, indent):\n   900\t        val = ''\n   901\t        is_classification = isinstance(decision_tree,\n   902\t                                       DecisionTreeClassifier)\n   903\t        if show_weights or not is_classification:\n   904\t            val = [\"{1:.{0}f}, \".format(decimals, v) for v in value]\n   905\t            val = '['+''.join(val)[:-2]+']'\n   906\t        if is_classification:\n   907\t            val += ' class: ' + str(class_name)\n   908\t        export_text.report += value_fmt.format(indent, '', val)\n   909\t\n   910\t    def print_tree_recurse(node, depth):\n   911\t        indent = (\"|\" + (\" \" * spacing)) * depth\n   912\t        indent = indent[:-spacing] + \"-\" * spacing\n   913\t\n   914\t        value = None\n   915\t        if tree_.n_outputs == 1:\n   916\t            value = tree_.value[node][0]\n   917\t        else:\n   918\t            value = tree_.value[node].T[0]\n   919\t        class_name = np.argmax(value)\n   920\t\n   921\t        if (tree_.n_classes[0] != 1 and\n   922\t                tree_.n_outputs == 1):\n   923\t            class_name = class_names[class_name]\n   924\t\n   925\t        if depth <= max_depth+1:\n   926\t            info_fmt = \"\"\n   927\t            info_fmt_left = info_fmt\n   928\t            info_fmt_right = info_fmt\n   929\t\n   930\t            if tree_.feature[node] != _tree.TREE_UNDEFINED:\n   931\t                name = feature_names_[node]\n   932\t                threshold = tree_.threshold[node]\n   933\t                threshold = \"{1:.{0}f}\".format(decimals, threshold)\n   934\t                export_text.report += right_child_fmt.format(indent,\n   935\t                                                             name,\n   936\t                                                             threshold)\n   937\t                export_text.report += info_fmt_left\n   938\t                print_tree_recurse(tree_.children_left[node], depth+1)\n   939\t\n   940\t                export_text.report += left_child_fmt.format(indent,\n   941\t                                                            name,\n   942\t                                                            threshold)\n   943\t                export_text.report += info_fmt_right\n   944\t                print_tree_recurse(tree_.children_right[node], depth+1)\n   945\t            else:  # leaf\n   946\t                _add_leaf(value, class_name, indent)\n   947\t        else:\n   948\t            subtree_depth = _compute_depth(tree_, node)\n   949\t            if subtree_depth == 1:\n   950\t                _add_leaf(value, class_name, indent)\n   951\t            else:\n   952\t                trunc_report = 'truncated branch of depth %d' % subtree_depth\n   953\t                export_text.report += truncation_fmt.format(indent,\n   954\t                                                            trunc_report)\n   955\t\n   956\t    print_tree_recurse(0, 1)\n   957\t    return export_text.report\n",
      "perturbed": "   760\t    try:\n   761\t        if isinstance(out_file, str):\n   762\t            out_file = open(out_file, \"w\", encoding=\"utf-8\")\n   763\t            own_file = True\n   764\t\n   765\t        if out_file is None:\n   766\t            return_string = True\n   768\t\n   769\t        exporter = _DOTTreeExporter(\n   770\t            out_file=out_file, max_depth=max_depth,\n   771\t            feature_names=feature_names, class_names=class_names, label=label,\n   772\t            filled=filled, leaves_parallel=leaves_parallel, impurity=impurity,\n   773\t            node_ids=node_ids, proportion=proportion, rotate=rotate,\n   777\t\n   774\t            rounded=rounded, special_characters=special_characters,\n   778\t        if return_string:\n   780\t\n   781\t    finally:\n   782\t        if own_file:\n   783\t            out_file.close()\n   785\t\n   784\t\n   786\tdef _compute_depth(tree, node):\n   787\t    \"\"\"\n   789\t    \"\"\"\n   788\t    Returns the depth of the subtree rooted in node.\n   790\t    def compute_depth_(current_node, current_depth,\n   791\t                       children_left, children_right, depths):\n   792\t        depths += [current_depth]\n   795\t        if left != -1 and right != -1:\n   794\t        right = children_right[current_node]\n   796\t            compute_depth_(left, current_depth+1,\n   797\t                           children_left, children_right, depths)\n   798\t            compute_depth_(right, current_depth+1,\n   799\t                           children_left, children_right, depths)\n   800\t\n   801\t    depths = []\n   803\t    return max(depths)\n   802\t    compute_depth_(node, 1, tree.children_left, tree.children_right, depths)\n   804\t\n   805\t\n   807\t                spacing=3, decimals=2, show_weights=False):\n   806\tdef export_text(decision_tree, feature_names=None, max_depth=10,\n   809\t\n   808\t    \"\"\"Build a text report showing the rules of a decision tree.\n   810\t    Note that backwards compatibility may not be supported.\n   811\t\n   815\t        The decision tree estimator to be exported.\n   814\t    decision_tree : object\n   816\t        It can be an instance of\n   817\t        DecisionTreeClassifier or DecisionTreeRegressor.\n   818\t\n   821\t        If None generic names will be used (\"feature_0\", \"feature_1\", ...).\n   819\t    feature_names : list, optional (default=None)\n   822\t\n   823\t    max_depth : int, optional (default=10)\n   826\t\n   827\t    spacing : int, optional (default=3)\n   828\t        Number of spaces between edges. The higher it is, the wider the result.\n   829\t\n   830\t    decimals : int, optional (default=2)\n   831\t        Number of decimal digits to display.\n   833\t    show_weights : bool, optional (default=False)\n   832\t\n   834\t        If true the classification weights will be exported on each leaf.\n   835\t        The classification weights are the number of samples each class.\n   836\t\n   837\t    Returns\n   840\t        Text summary of all the rules in the decision tree.\n   841\t\n   842\t    Examples\n   843\t    --------\n   845\t    >>> from sklearn.datasets import load_iris\n   844\t\n   847\t    >>> from sklearn.tree.export import export_text\n   846\t    >>> from sklearn.tree import DecisionTreeClassifier\n   849\t    >>> X = iris['data']\n   850\t    >>> y = iris['target']\n   852\t    >>> decision_tree = decision_tree.fit(X, y)\n   853\t    >>> r = export_text(decision_tree, feature_names=iris['feature_names'])\n   854\t    >>> print(r)\n   855\t    |--- petal width (cm) <= 0.80\n   857\t    |--- petal width (cm) >  0.80\n   856\t    |   |--- class: 0\n   859\t    |   |   |--- class: 1\n   858\t    |   |--- petal width (cm) <= 1.75\n   861\t    |   |   |--- class: 2\n   860\t    |   |--- petal width (cm) >  1.75\n   862\t    \"\"\"\n   863\t    check_is_fitted(decision_tree, 'tree_')\n   864\t    tree_ = decision_tree.tree_\n   865\t    class_names = decision_tree.classes_\n   867\t    left_child_fmt = \"{} {} >  {}\\n\"\n   866\t    right_child_fmt = \"{} {} <= {}\\n\"\n   868\t    truncation_fmt = \"{} {}\\n\"\n   869\t\n   871\t        raise ValueError(\"max_depth bust be >= 0, given %d\" % max_depth)\n   870\t    if max_depth < 0:\n   872\t\n   874\t            len(feature_names) != tree_.n_features):\n   876\t                         \"%d elements, got %d\" % (tree_.n_features,\n   875\t        raise ValueError(\"feature_names must contain \"\n   877\t                                                  len(feature_names)))\n   878\t\n   880\t        raise ValueError(\"spacing must be > 0, given %d\" % spacing)\n   879\t    if spacing <= 0:\n   882\t    if decimals < 0:\n   881\t\n   883\t        raise ValueError(\"decimals must be >= 0, given %d\" % decimals)\n   885\t    if isinstance(decision_tree, DecisionTreeClassifier):\n   886\t        value_fmt = \"{}{} weights: {}\\n\"\n   887\t        if not show_weights:\n   889\t    else:\n   890\t        value_fmt = \"{}{} value: {}\\n\"\n   891\t\n   892\t    if feature_names:\n   893\t        feature_names_ = [feature_names[i] for i in tree_.feature]\n   894\t    else:\n   895\t        feature_names_ = [\"feature_{}\".format(i) for i in tree_.feature]\n   896\t\n   897\t    export_text.report = \"\"\n   899\t    def _add_leaf(value, class_name, indent):\n   898\t\n   900\t        val = ''\n   903\t        if show_weights or not is_classification:\n   902\t                                       DecisionTreeClassifier)\n   905\t            val = '['+''.join(val)[:-2]+']'\n   908\t        export_text.report += value_fmt.format(indent, '', val)\n   907\t            val += ' class: ' + str(class_name)\n   909\t\n   912\t        indent = indent[:-spacing] + \"-\" * spacing\n   910\t    def print_tree_recurse(node, depth):\n   914\t        value = None\n   913\t\n   915\t        if tree_.n_outputs == 1:\n   916\t            value = tree_.value[node][0]\n   919\t        class_name = np.argmax(value)\n   917\t        else:\n   920\t\n   921\t        if (tree_.n_classes[0] != 1 and\n   922\t                tree_.n_outputs == 1):\n   924\t\n   927\t            info_fmt_left = info_fmt\n   928\t            info_fmt_right = info_fmt\n   929\t\n   930\t            if tree_.feature[node] != _tree.TREE_UNDEFINED:\n   933\t                threshold = \"{1:.{0}f}\".format(decimals, threshold)\n   934\t                export_text.report += right_child_fmt.format(indent,\n   935\t                                                             name,\n   938\t                print_tree_recurse(tree_.children_left[node], depth+1)\n   936\t                                                             threshold)\n   939\t\n   940\t                export_text.report += left_child_fmt.format(indent,\n   941\t                                                            name,\n   942\t                                                            threshold)\n   944\t                print_tree_recurse(tree_.children_right[node], depth+1)\n   945\t            else:  # leaf\n   947\t        else:\n   946\t                _add_leaf(value, class_name, indent)\n   949\t            if subtree_depth == 1:\n   950\t                _add_leaf(value, class_name, indent)\n   951\t            else:\n   952\t                trunc_report = 'truncated branch of depth %d' % subtree_depth\n   953\t                export_text.report += truncation_fmt.format(indent,\n   954\t                                                            trunc_report)\n   955\t\n   956\t    print_tree_recurse(0, 1)\n   957\t    return export_text.report",
      "applied_operators": [
        "text_perturbation"
      ]
    },
    {
      "action": "nl -ba sklearn/tree/tests/test_export.py | sed -n '300,420p'",
      "original": "   300\t            else:\n   301\t                pattern = r\"friedman_mse = \\d+\\.\\d+\"\n   302\t\n   303\t            # check impurity\n   304\t            for finding in finditer(pattern, dot_data):\n   305\t                assert_equal(len(search(r\"\\.\\d+\", finding.group()).group()),\n   306\t                             precision + 1)\n   307\t            # check threshold\n   308\t            for finding in finditer(r\"<= \\d+\\.\\d+\", dot_data):\n   309\t                assert_equal(len(search(r\"\\.\\d+\", finding.group()).group()),\n   310\t                             precision + 1)\n   311\t\n   312\t\n   313\tdef test_export_text_errors():\n   314\t    clf = DecisionTreeClassifier(max_depth=2, random_state=0)\n   315\t    clf.fit(X, y)\n   316\t\n   317\t    assert_raise_message(ValueError,\n   318\t                         \"max_depth bust be >= 0, given -1\",\n   319\t                         export_text, clf, max_depth=-1)\n   320\t    assert_raise_message(ValueError,\n   321\t                         \"feature_names must contain 2 elements, got 1\",\n   322\t                         export_text, clf, feature_names=['a'])\n   323\t    assert_raise_message(ValueError,\n   324\t                         \"decimals must be >= 0, given -1\",\n   325\t                         export_text, clf, decimals=-1)\n   326\t    assert_raise_message(ValueError,\n   327\t                         \"spacing must be > 0, given 0\",\n   328\t                         export_text, clf, spacing=0)\n   329\t\n   330\t\n   331\tdef test_export_text():\n   332\t    clf = DecisionTreeClassifier(max_depth=2, random_state=0)\n   333\t    clf.fit(X, y)\n   334\t\n   335\t    expected_report = dedent(\"\"\"\n   336\t    |--- feature_1 <= 0.00\n   337\t    |   |--- class: -1\n   338\t    |--- feature_1 >  0.00\n   339\t    |   |--- class: 1\n   340\t    \"\"\").lstrip()\n   341\t\n   342\t    assert export_text(clf) == expected_report\n   343\t    # testing that leaves at level 1 are not truncated\n   344\t    assert export_text(clf, max_depth=0) == expected_report\n   345\t    # testing that the rest of the tree is truncated\n   346\t    assert export_text(clf, max_depth=10) == expected_report\n   347\t\n   348\t    expected_report = dedent(\"\"\"\n   349\t    |--- b <= 0.00\n   350\t    |   |--- class: -1\n   351\t    |--- b >  0.00\n   352\t    |   |--- class: 1\n   353\t    \"\"\").lstrip()\n   354\t    assert export_text(clf, feature_names=['a', 'b']) == expected_report\n   355\t\n   356\t    expected_report = dedent(\"\"\"\n   357\t    |--- feature_1 <= 0.00\n   358\t    |   |--- weights: [3.00, 0.00] class: -1\n   359\t    |--- feature_1 >  0.00\n   360\t    |   |--- weights: [0.00, 3.00] class: 1\n   361\t    \"\"\").lstrip()\n   362\t    assert export_text(clf, show_weights=True) == expected_report\n   363\t\n   364\t    expected_report = dedent(\"\"\"\n   365\t    |- feature_1 <= 0.00\n   366\t    | |- class: -1\n   367\t    |- feature_1 >  0.00\n   368\t    | |- class: 1\n   369\t    \"\"\").lstrip()\n   370\t    assert export_text(clf, spacing=1) == expected_report\n   371\t\n   372\t    X_l = [[-2, -1], [-1, -1], [-1, -2], [1, 1], [1, 2], [2, 1], [-1, 1]]\n   373\t    y_l = [-1, -1, -1, 1, 1, 1, 2]\n   374\t    clf = DecisionTreeClassifier(max_depth=4, random_state=0)\n   375\t    clf.fit(X_l, y_l)\n   376\t    expected_report = dedent(\"\"\"\n   377\t    |--- feature_1 <= 0.00\n   378\t    |   |--- class: -1\n   379\t    |--- feature_1 >  0.00\n   380\t    |   |--- truncated branch of depth 2\n   381\t    \"\"\").lstrip()\n   382\t    assert export_text(clf, max_depth=0) == expected_report\n   383\t\n   384\t    X_mo = [[-2, -1], [-1, -1], [-1, -2], [1, 1], [1, 2], [2, 1]]\n   385\t    y_mo = [[-1, -1], [-1, -1], [-1, -1], [1, 1], [1, 1], [1, 1]]\n   386\t\n   387\t    reg = DecisionTreeRegressor(max_depth=2, random_state=0)\n   388\t    reg.fit(X_mo, y_mo)\n   389\t\n   390\t    expected_report = dedent(\"\"\"\n   391\t    |--- feature_1 <= 0.0\n   392\t    |   |--- value: [-1.0, -1.0]\n   393\t    |--- feature_1 >  0.0\n   394\t    |   |--- value: [1.0, 1.0]\n   395\t    \"\"\").lstrip()\n   396\t    assert export_text(reg, decimals=1) == expected_report\n   397\t    assert export_text(reg, decimals=1, show_weights=True) == expected_report\n   398\t\n   399\t\n   400\tdef test_plot_tree_entropy(pyplot):\n   401\t    # mostly smoke tests\n   402\t    # Check correctness of export_graphviz for criterion = entropy\n   403\t    clf = DecisionTreeClassifier(max_depth=3,\n   404\t                                 min_samples_split=2,\n   405\t                                 criterion=\"entropy\",\n   406\t                                 random_state=2)\n   407\t    clf.fit(X, y)\n   408\t\n   409\t    # Test export code\n   410\t    feature_names = ['first feat', 'sepal_width']\n   411\t    nodes = plot_tree(clf, feature_names=feature_names)\n   412\t    assert len(nodes) == 3\n   413\t    assert nodes[0].get_text() == (\"first feat <= 0.0\\nentropy = 1.0\\n\"\n   414\t                                   \"samples = 6\\nvalue = [3, 3]\")\n   415\t    assert nodes[1].get_text() == \"entropy = 0.0\\nsamples = 3\\nvalue = [3, 0]\"\n   416\t    assert nodes[2].get_text() == \"entropy = 0.0\\nsamples = 3\\nvalue = [0, 3]\"\n   417\t\n   418\t\n   419\tdef test_plot_tree_gini(pyplot):\n   420\t    # mostly smoke tests\n",
      "perturbed": "   300\t            else:\n   301\t                pattern = r\"friedman_mse = \\d+\\.\\d+\"\n   302\t\n   303\t            # check impurity\n   304\t            for finding in finditer(pattern, dot_data):\n   306\t                             precision + 1)\n   305\t                assert_equal(len(search(r\"\\.\\d+\", finding.group()).group()),\n   308\t            for finding in finditer(r\"<= \\d+\\.\\d+\", dot_data):\n   309\t                assert_equal(len(search(r\"\\.\\d+\", finding.group()).group()),\n   310\t                             precision + 1)\n   311\t\n   312\t\n   313\tdef test_export_text_errors():\n   314\t    clf = DecisionTreeClassifier(max_depth=2, random_state=0)\n   315\t    clf.fit(X, y)\n   317\t    assert_raise_message(ValueError,\n   316\t\n   319\t                         export_text, clf, max_depth=-1)\n   320\t    assert_raise_message(ValueError,\n   322\t                         export_text, clf, feature_names=['a'])\n   323\t    assert_raise_message(ValueError,\n   324\t                         \"decimals must be >= 0, given -1\",\n   325\t                         export_text, clf, decimals=-1)\n   327\t                         \"spacing must be > 0, given 0\",\n   326\t    assert_raise_message(ValueError,\n   329\t\n   330\t\n   332\t    clf = DecisionTreeClassifier(max_depth=2, random_state=0)\n   331\tdef test_export_text():\n   333\t    clf.fit(X, y)\n   334\t\n   335\t    expected_report = dedent(\"\"\"\n   336\t    |--- feature_1 <= 0.00\n   337\t    |   |--- class: -1\n   338\t    |--- feature_1 >  0.00\n   339\t    |   |--- class: 1\n   340\t    \"\"\").lstrip()\n   341\t\n   342\t    assert export_text(clf) == expected_report\n   343\t    # testing that leaves at level 1 are not truncated\n   344\t    assert export_text(clf, max_depth=0) == expected_report\n   345\t    # testing that the rest of the tree is truncated\n   346\t    assert export_text(clf, max_depth=10) == expected_report\n   347\t\n   348\t    expected_report = dedent(\"\"\"\n   349\t    |--- b <= 0.00\n   353\t    \"\"\").lstrip()\n   351\t    |--- b >  0.00\n   354\t    assert export_text(clf, feature_names=['a', 'b']) == expected_report\n   356\t    expected_report = dedent(\"\"\"\n   355\t\n   358\t    |   |--- weights: [3.00, 0.00] class: -1\n   357\t    |--- feature_1 <= 0.00\n   359\t    |--- feature_1 >  0.00\n   361\t    \"\"\").lstrip()\n   360\t    |   |--- weights: [0.00, 3.00] class: 1\n   363\t\n   364\t    expected_report = dedent(\"\"\"\n   365\t    |- feature_1 <= 0.00\n   366\t    | |- class: -1\n   368\t    | |- class: 1\n   367\t    |- feature_1 >  0.00\n   370\t    assert export_text(clf, spacing=1) == expected_report\n   369\t    \"\"\").lstrip()\n   371\t\n   372\t    X_l = [[-2, -1], [-1, -1], [-1, -2], [1, 1], [1, 2], [2, 1], [-1, 1]]\n   373\t    y_l = [-1, -1, -1, 1, 1, 1, 2]\n   374\t    clf = DecisionTreeClassifier(max_depth=4, random_state=0)\n   375\t    clf.fit(X_l, y_l)\n   376\t    expected_report = dedent(\"\"\"\n   377\t    |--- feature_1 <= 0.00\n   378\t    |   |--- class: -1\n   379\t    |--- feature_1 >  0.00\n   383\t\n   381\t    \"\"\").lstrip()\n   385\t    y_mo = [[-1, -1], [-1, -1], [-1, -1], [1, 1], [1, 1], [1, 1]]\n   384\t    X_mo = [[-2, -1], [-1, -1], [-1, -2], [1, 1], [1, 2], [2, 1]]\n   386\t\n   387\t    reg = DecisionTreeRegressor(max_depth=2, random_state=0)\n   389\t\n   388\t    reg.fit(X_mo, y_mo)\n   391\t    |--- feature_1 <= 0.0\n   392\t    |   |--- value: [-1.0, -1.0]\n   395\t    \"\"\").lstrip()\n   393\t    |--- feature_1 >  0.0\n   396\t    assert export_text(reg, decimals=1) == expected_report\n   397\t    assert export_text(reg, decimals=1, show_weights=True) == expected_report\n   399\t\n   401\t    # mostly smoke tests\n   400\tdef test_plot_tree_entropy(pyplot):\n   402\t    # Check correctness of export_graphviz for criterion = entropy\n   403\t    clf = DecisionTreeClassifier(max_depth=3,\n   404\t                                 min_samples_split=2,\n   405\t                                 criterion=\"entropy\",\n   406\t                                 random_state=2)\n   407\t    clf.fit(X, y)\n   409\t    # Test export code\n   410\t    feature_names = ['first feat', 'sepal_width']\n   411\t    nodes = plot_tree(clf, feature_names=feature_names)\n   412\t    assert len(nodes) == 3\n   413\t    assert nodes[0].get_text() == (\"first feat <= 0.0\\nentropy = 1.0\\n\"\n   415\t    assert nodes[1].get_text() == \"entropy = 0.0\\nsamples = 3\\nvalue = [3, 0]\"\n   414\t                                   \"samples = 6\\nvalue = [3, 3]\")\n   418\t\n   416\t    assert nodes[2].get_text() == \"entropy = 0.0\\nsamples = 3\\nvalue = [0, 3]\"\n   419\tdef test_plot_tree_gini(pyplot):\n   420\t    # mostly smoke tests",
      "applied_operators": [
        "text_perturbation"
      ]
    },
    {
      "action": "grep -R \"TREE_UNDEFINED\" -n sklearn | sed -n '1,120p'",
      "original": "grep: sklearn/tree/_tree.cpython-36m-x86_64-linux-gnu.so: binary file matches\nsklearn/tree/_tree.pyx:64:TREE_UNDEFINED = -2\nsklearn/tree/_tree.pyx:66:cdef SIZE_t _TREE_UNDEFINED = TREE_UNDEFINED\nsklearn/tree/_tree.pyx:202:            rc = stack.push(0, n_node_samples, 0, _TREE_UNDEFINED, 0, INFINITY, 0)\nsklearn/tree/_tree.pyx:375:                    node.feature = _TREE_UNDEFINED\nsklearn/tree/_tree.pyx:376:                    node.threshold = _TREE_UNDEFINED\nsklearn/tree/_tree.pyx:467:                                 else _TREE_UNDEFINED,\nsklearn/tree/_tree.pyx:748:        if parent != _TREE_UNDEFINED:\nsklearn/tree/_tree.pyx:757:            node.feature = _TREE_UNDEFINED\nsklearn/tree/_tree.pyx:758:            node.threshold = _TREE_UNDEFINED\nsklearn/tree/export.py:930:            if tree_.feature[node] != _tree.TREE_UNDEFINED:\nsklearn/tree/_tree.c:3286:static __pyx_t_7sklearn_4tree_5_tree_SIZE_t __pyx_v_7sklearn_4tree_5_tree__TREE_UNDEFINED;\nsklearn/tree/_tree.c:3473:static const char __pyx_k_TREE_UNDEFINED[] = \"TREE_UNDEFINED\";\nsklearn/tree/_tree.c:3563:static PyObject *__pyx_n_s_TREE_UNDEFINED;\nsklearn/tree/_tree.c:5786: *             rc = stack.push(0, n_node_samples, 0, _TREE_UNDEFINED, 0, INFINITY, 0)\nsklearn/tree/_tree.c:5799: *             rc = stack.push(0, n_node_samples, 0, _TREE_UNDEFINED, 0, INFINITY, 0)             # <<<<<<<<<<<<<<\nsklearn/tree/_tree.c:5803:        __pyx_t_5 = ((struct __pyx_vtabstruct_7sklearn_4tree_6_utils_Stack *)__pyx_v_stack->__pyx_vtab)->push(__pyx_v_stack, 0, __pyx_v_n_node_samples, 0, __pyx_v_7sklearn_4tree_5_tree__TREE_UNDEFINED, 0, __pyx_v_7sklearn_4tree_5_tree_INFINITY, 0); if (unlikely(__pyx_t_5 == ((int)-1))) __PYX_ERR(0, 202, __pyx_L8_error)\nsklearn/tree/_tree.c:5808: *             rc = stack.push(0, n_node_samples, 0, _TREE_UNDEFINED, 0, INFINITY, 0)\nsklearn/tree/_tree.c:5858: *             rc = stack.push(0, n_node_samples, 0, _TREE_UNDEFINED, 0, INFINITY, 0)\nsklearn/tree/_tree.c:6396: *             rc = stack.push(0, n_node_samples, 0, _TREE_UNDEFINED, 0, INFINITY, 0)\nsklearn/tree/_tree.c:7645: *                     node.feature = _TREE_UNDEFINED\nsklearn/tree/_tree.c:7653: *                     node.feature = _TREE_UNDEFINED\nsklearn/tree/_tree.c:7654: *                     node.threshold = _TREE_UNDEFINED\nsklearn/tree/_tree.c:7661: *                     node.feature = _TREE_UNDEFINED             # <<<<<<<<<<<<<<\nsklearn/tree/_tree.c:7662: *                     node.threshold = _TREE_UNDEFINED\nsklearn/tree/_tree.c:7665:            __pyx_v_node->feature = __pyx_v_7sklearn_4tree_5_tree__TREE_UNDEFINED;\nsklearn/tree/_tree.c:7669: *                     node.feature = _TREE_UNDEFINED\nsklearn/tree/_tree.c:7670: *                     node.threshold = _TREE_UNDEFINED             # <<<<<<<<<<<<<<\nsklearn/tree/_tree.c:7674:            __pyx_v_node->threshold = __pyx_v_7sklearn_4tree_5_tree__TREE_UNDEFINED;\nsklearn/tree/_tree.c:8447: *                                  else _TREE_UNDEFINED,\nsklearn/tree/_tree.c:8457: *                                  else _TREE_UNDEFINED,\nsklearn/tree/_tree.c:8465: *                                  else _TREE_UNDEFINED,             # <<<<<<<<<<<<<<\nsklearn/tree/_tree.c:8469:    __pyx_t_5 = __pyx_v_7sklearn_4tree_5_tree__TREE_UNDEFINED;\nsklearn/tree/_tree.c:8477: *                                  else _TREE_UNDEFINED,\nsklearn/tree/_tree.c:11182: *         if parent != _TREE_UNDEFINED:\nsklearn/tree/_tree.c:11189: *         if parent != _TREE_UNDEFINED:             # <<<<<<<<<<<<<<\nsklearn/tree/_tree.c:11193:  __pyx_t_2 = ((__pyx_v_parent != __pyx_v_7sklearn_4tree_5_tree__TREE_UNDEFINED) != 0);\nsklearn/tree/_tree.c:11198: *         if parent != _TREE_UNDEFINED:\nsklearn/tree/_tree.c:11207: *         if parent != _TREE_UNDEFINED:\nsklearn/tree/_tree.c:11217: *         if parent != _TREE_UNDEFINED:\nsklearn/tree/_tree.c:11240: *         if parent != _TREE_UNDEFINED:             # <<<<<<<<<<<<<<\nsklearn/tree/_tree.c:11261: *             node.feature = _TREE_UNDEFINED\nsklearn/tree/_tree.c:11269: *             node.feature = _TREE_UNDEFINED\nsklearn/tree/_tree.c:11270: *             node.threshold = _TREE_UNDEFINED\nsklearn/tree/_tree.c:11277: *             node.feature = _TREE_UNDEFINED             # <<<<<<<<<<<<<<\nsklearn/tree/_tree.c:11278: *             node.threshold = _TREE_UNDEFINED\nsklearn/tree/_tree.c:11281:    __pyx_v_node->feature = __pyx_v_7sklearn_4tree_5_tree__TREE_UNDEFINED;\nsklearn/tree/_tree.c:11285: *             node.feature = _TREE_UNDEFINED\nsklearn/tree/_tree.c:11286: *             node.threshold = _TREE_UNDEFINED             # <<<<<<<<<<<<<<\nsklearn/tree/_tree.c:11290:    __pyx_v_node->threshold = __pyx_v_7sklearn_4tree_5_tree__TREE_UNDEFINED;\nsklearn/tree/_tree.c:32552:  {&__pyx_n_s_TREE_UNDEFINED, __pyx_k_TREE_UNDEFINED, sizeof(__pyx_k_TREE_UNDEFINED), 0, 0, 1, 1},\nsklearn/tree/_tree.c:33845: * TREE_UNDEFINED = -2\nsklearn/tree/_tree.c:33853: * TREE_UNDEFINED = -2             # <<<<<<<<<<<<<<\nsklearn/tree/_tree.c:33855: * cdef SIZE_t _TREE_UNDEFINED = TREE_UNDEFINED\nsklearn/tree/_tree.c:33857:  if (PyDict_SetItem(__pyx_d, __pyx_n_s_TREE_UNDEFINED, __pyx_int_neg_2) < 0) __PYX_ERR(0, 64, __pyx_L1_error)\nsklearn/tree/_tree.c:33861: * TREE_UNDEFINED = -2\nsklearn/tree/_tree.c:33863: * cdef SIZE_t _TREE_UNDEFINED = TREE_UNDEFINED\nsklearn/tree/_tree.c:33873: * TREE_UNDEFINED = -2\nsklearn/tree/_tree.c:33875: * cdef SIZE_t _TREE_UNDEFINED = TREE_UNDEFINED             # <<<<<<<<<<<<<<\nsklearn/tree/_tree.c:33879:  __Pyx_GetModuleGlobalName(__pyx_t_3, __pyx_n_s_TREE_UNDEFINED); if (unlikely(!__pyx_t_3)) __PYX_ERR(0, 66, __pyx_L1_error)\nsklearn/tree/_tree.c:33883:  __pyx_v_7sklearn_4tree_5_tree__TREE_UNDEFINED = __pyx_t_5;\nsklearn/tree/_tree.c:33887: * cdef SIZE_t _TREE_UNDEFINED = TREE_UNDEFINED\n",
      "perturbed": "grep: sklearn/tree/_tree.cpython-36m-x86_64-linux-gnu.so: binary file matches\nsklearn/tree/_tree.pyx:64:TREE_UNDEFINED = -2\nsklearn/tree/_tree.pyx:66:cdef SIZE_t _TREE_UNDEFINED = TREE_UNDEFINED\nsklearn/tree/_tree.pyx:202:            rc = stack.push(0, n_node_samples, 0, _TREE_UNDEFINED, 0, INFINITY, 0)\nsklearn/tree/_tree.pyx:375:                    node.feature = _TREE_UNDEFINED\nsklearn/tree/_tree.pyx:376:                    node.threshold = _TREE_UNDEFINED\nsklearn/tree/_tree.pyx:467:                                 else _TREE_UNDEFINED,\nsklearn/tree/_tree.pyx:748:        if parent != _TREE_UNDEFINED:\nsklearn/tree/_tree.pyx:757:            node.feature = _TREE_UNDEFINED\nsklearn/tree/export.py:930:            if tree_.feature[node] != _tree.TREE_UNDEFINED:\nsklearn/tree/_tree.c:3563:static PyObject *__pyx_n_s_TREE_UNDEFINED;\nsklearn/tree/_tree.c:5786: *             rc = stack.push(0, n_node_samples, 0, _TREE_UNDEFINED, 0, INFINITY, 0)\nsklearn/tree/_tree.c:5799: *             rc = stack.push(0, n_node_samples, 0, _TREE_UNDEFINED, 0, INFINITY, 0)             # <<<<<<<<<<<<<<\nsklearn/tree/_tree.c:5808: *             rc = stack.push(0, n_node_samples, 0, _TREE_UNDEFINED, 0, INFINITY, 0)\nsklearn/tree/_tree.c:5858: *             rc = stack.push(0, n_node_samples, 0, _TREE_UNDEFINED, 0, INFINITY, 0)\nsklearn/tree/_tree.c:7653: *                     node.feature = _TREE_UNDEFINED\nsklearn/tree/_tree.c:7645: *                     node.feature = _TREE_UNDEFINED\nsklearn/tree/_tree.c:7654: *                     node.threshold = _TREE_UNDEFINED\nsklearn/tree/_tree.c:7661: *                     node.feature = _TREE_UNDEFINED             # <<<<<<<<<<<<<<\nsklearn/tree/_tree.c:7662: *                     node.threshold = _TREE_UNDEFINED\nsklearn/tree/_tree.c:7665:            __pyx_v_node->feature = __pyx_v_7sklearn_4tree_5_tree__TREE_UNDEFINED;\nsklearn/tree/_tree.c:7669: *                     node.feature = _TREE_UNDEFINED\nsklearn/tree/_tree.c:7670: *                     node.threshold = _TREE_UNDEFINED             # <<<<<<<<<<<<<<\nsklearn/tree/_tree.c:8447: *                                  else _TREE_UNDEFINED,\nsklearn/tree/_tree.c:8457: *                                  else _TREE_UNDEFINED,\nsklearn/tree/_tree.c:8469:    __pyx_t_5 = __pyx_v_7sklearn_4tree_5_tree__TREE_UNDEFINED;\nsklearn/tree/_tree.c:11182: *         if parent != _TREE_UNDEFINED:\nsklearn/tree/_tree.c:8477: *                                  else _TREE_UNDEFINED,\nsklearn/tree/_tree.c:11189: *         if parent != _TREE_UNDEFINED:             # <<<<<<<<<<<<<<\nsklearn/tree/_tree.c:11193:  __pyx_t_2 = ((__pyx_v_parent != __pyx_v_7sklearn_4tree_5_tree__TREE_UNDEFINED) != 0);\nsklearn/tree/_tree.c:11207: *         if parent != _TREE_UNDEFINED:\nsklearn/tree/_tree.c:11198: *         if parent != _TREE_UNDEFINED:\nsklearn/tree/_tree.c:11217: *         if parent != _TREE_UNDEFINED:\nsklearn/tree/_tree.c:11240: *         if parent != _TREE_UNDEFINED:             # <<<<<<<<<<<<<<\nsklearn/tree/_tree.c:11261: *             node.feature = _TREE_UNDEFINED\nsklearn/tree/_tree.c:11269: *             node.feature = _TREE_UNDEFINED\nsklearn/tree/_tree.c:11270: *             node.threshold = _TREE_UNDEFINED\nsklearn/tree/_tree.c:11277: *             node.feature = _TREE_UNDEFINED             # <<<<<<<<<<<<<<\nsklearn/tree/_tree.c:11281:    __pyx_v_node->feature = __pyx_v_7sklearn_4tree_5_tree__TREE_UNDEFINED;\nsklearn/tree/_tree.c:11278: *             node.threshold = _TREE_UNDEFINED\nsklearn/tree/_tree.c:11285: *             node.feature = _TREE_UNDEFINED\nsklearn/tree/_tree.c:11286: *             node.threshold = _TREE_UNDEFINED             # <<<<<<<<<<<<<<\nsklearn/tree/_tree.c:32552:  {&__pyx_n_s_TREE_UNDEFINED, __pyx_k_TREE_UNDEFINED, sizeof(__pyx_k_TREE_UNDEFINED), 0, 0, 1, 1},\nsklearn/tree/_tree.c:11290:    __pyx_v_node->threshold = __pyx_v_7sklearn_4tree_5_tree__TREE_UNDEFINED;\nsklearn/tree/_tree.c:33845: * TREE_UNDEFINED = -2\nsklearn/tree/_tree.c:33853: * TREE_UNDEFINED = -2             # <<<<<<<<<<<<<<\nsklearn/tree/_tree.c:33855: * cdef SIZE_t _TREE_UNDEFINED = TREE_UNDEFINED\nsklearn/tree/_tree.c:33857:  if (PyDict_SetItem(__pyx_d, __pyx_n_s_TREE_UNDEFINED, __pyx_int_neg_2) < 0) __PYX_ERR(0, 64, __pyx_L1_error)\nsklearn/tree/_tree.c:33861: * TREE_UNDEFINED = -2\nsklearn/tree/_tree.c:33863: * cdef SIZE_t _TREE_UNDEFINED = TREE_UNDEFINED\nsklearn/tree/_tree.c:33873: * TREE_UNDEFINED = -2\nsklearn/tree/_tree.c:33875: * cdef SIZE_t _TREE_UNDEFINED = TREE_UNDEFINED             # <<<<<<<<<<<<<<\nsklearn/tree/_tree.c:33879:  __Pyx_GetModuleGlobalName(__pyx_t_3, __pyx_n_s_TREE_UNDEFINED); if (unlikely(!__pyx_t_3)) __PYX_ERR(0, 66, __pyx_L1_error)\nsklearn/tree/_tree.c:33883:  __pyx_v_7sklearn_4tree_5_tree__TREE_UNDEFINED = __pyx_t_5;\nsklearn/tree/_tree.c:33887: * cdef SIZE_t _TREE_UNDEFINED = TREE_UNDEFINED",
      "applied_operators": [
        "text_perturbation"
      ]
    },
    {
      "action": "PYTHONPATH=$(pwd) pytest -q sklearn/tree/tests/test_export.py::test_export_text sklearn/tree/tests/test_export.py::test_export_text_errors",
      "original": "..                                                                       [100%]\n2 passed, 1 warning in 0.18s\n",
      "perturbed": "2 passed, 1 warning in 0.18s",
      "applied_operators": [
        "text_perturbation"
      ]
    }
  ]
}
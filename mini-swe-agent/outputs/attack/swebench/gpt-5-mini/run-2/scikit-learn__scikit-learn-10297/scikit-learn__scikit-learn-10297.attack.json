{
  "info": {
    "total_observations": 13,
    "python_observations": 0,
    "text_observations": 13,
    "ast_operators_applied": 0,
    "text_perturbation_applied": 12,
    "skipped": 1
  },
  "perturbations": [
    {
      "action": "ls -la",
      "original": "total 152\ndrwxrwxrwx  1 root root  4096 Sep 10 15:16 .\ndrwxr-xr-x  1 root root  4096 Nov 30 01:40 ..\ndrwxrwxrwx  2 root root  4096 Sep 10 15:12 .circleci\n-rw-r--r--  1 root root   672 Sep 10 15:12 .codecov.yml\n-rw-r--r--  1 root root   126 Sep 10 15:12 .coveragerc\ndrwxrwxrwx  8 root root  4096 Sep 10 15:19 .git\n-rw-r--r--  1 root root    31 Sep 10 15:12 .gitattributes\n-rw-r--r--  1 root root   694 Sep 10 15:12 .gitignore\n-rw-r--r--  1 root root    86 Sep 10 15:12 .landscape.yml\n-rw-r--r--  1 root root  7263 Sep 10 15:12 .mailmap\n-rw-r--r--  1 root root  2561 Sep 10 15:12 .travis.yml\n-rw-r--r--  1 root root  2703 Sep 10 15:12 AUTHORS.rst\n-rw-r--r--  1 root root 10405 Sep 10 15:12 CONTRIBUTING.md\n-rw-r--r--  1 root root  1559 Sep 10 15:12 COPYING\n-rw-r--r--  1 root root  1768 Sep 10 15:12 ISSUE_TEMPLATE.md\n-rw-r--r--  1 root root   243 Sep 10 15:12 MANIFEST.in\n-rw-r--r--  1 root root  1375 Sep 10 15:12 Makefile\n-rw-r--r--  1 root root  1198 Sep 10 15:12 PULL_REQUEST_TEMPLATE.md\n-rw-r--r--  1 root root  5507 Sep 10 15:12 README.rst\n-rw-r--r--  1 root root  4198 Sep 10 15:12 appveyor.yml\ndrwxrwxrwx  2 root root  4096 Sep 10 15:12 benchmarks\ndrwxr-xr-x  3 root root  4096 Sep 10 15:16 build\ndrwxrwxrwx  6 root root  4096 Sep 10 15:12 build_tools\n-rw-r--r--  1 root root   542 Sep 10 15:12 conftest.py\ndrwxrwxrwx 14 root root  4096 Sep 10 15:12 doc\ndrwxrwxrwx 27 root root  4096 Sep 10 15:12 examples\ndrwxr-xr-x  2 root root  4096 Sep 10 15:16 scikit_learn.egg-info\n-rw-r--r--  1 root root   769 Sep 10 15:12 setup.cfg\n-rwxr-xr-x  1 root root  9225 Sep 10 15:12 setup.py\n-rw-r--r--  1 root root   292 Sep 10 15:12 site.cfg\ndrwxrwxrwx 29 root root  4096 Sep 10 15:18 sklearn\n",
      "perturbed": "drwxrwxrwx  1 root root  4096 Sep 10 15:16 .\ntotal 152\ndrwxr-xr-x  1 root root  4096 Nov 30 01:40 ..\ndrwxrwxrwx  2 root root  4096 Sep 10 15:12 .circleci\n-rw-r--r--  1 root root   672 Sep 10 15:12 .codecov.yml\ndrwxrwxrwx  8 root root  4096 Sep 10 15:19 .git\n-rw-r--r--  1 root root   694 Sep 10 15:12 .gitignore\n-rw-r--r--  1 root root    86 Sep 10 15:12 .landscape.yml\n-rw-r--r--  1 root root  7263 Sep 10 15:12 .mailmap\n-rw-r--r--  1 root root  2561 ...\n-rw-r--r--  1 root root  2703 Sep 10 15:12 AUTHORS.rst\n-rw-r--r--  1 root root 10405 Sep 10 15:12 CONTRIBUTING.md\n-rw-r--r--  1 root root  1559 Sep 10 15:12 COPYING\n-rw-r--r--  1 root root  1768 Sep 10 15:12 ISSUE_TEMPLATE.md\n-rw-r--r--  1 root root   243 Sep 10 15:12 MANIFEST.in\n-rw-r--r--  1 root root  1198 Sep 10 15:12 PULL_REQUEST_TEMPLATE.md\n-rw-r--r--  1 root root  5507 Sep 10 15:12 README.rst\ndrwxr-xr-x  3 root root  4096 Sep 10 15:16 build\ndrwxrwxrwx  6 root root  4096 Sep 10 15:12 build_tools\n-rw-r--r--  1 root root   542 Sep 10 15:12 conftest.py\ndrwxrwxrwx 14 root root  4096 Sep 10 15:12 doc\ndrwxr-xr-x  2 root root  4096 Sep 10 15:16 scikit_learn.egg-info\ndrwxrwxrwx 27 root root  4096 Sep 10 15:12 examples\n-rw-r--r--  1 root root   769 Sep 10 15:12 setup.cfg\n-rwxr-xr-x  1 root root  9225 Sep 10 15:12 setup.py\n-rw-r--r--  1 root root   292 Sep 10 15:12 site.cfg",
      "applied_operators": [
        "text_perturbation"
      ]
    },
    {
      "action": "grep -R \"class RidgeClassifierCV\" -n",
      "original": "sklearn/linear_model/ridge.py:1247:class RidgeClassifierCV(LinearClassifierMixin, _BaseRidgeCV):\n",
      "perturbed": "sklearn/linear_model/ridg...\n# additional context omitted",
      "applied_operators": [
        "text_perturbation"
      ]
    },
    {
      "action": "nl -ba sklearn/linear_model/ridge.py | sed -n '1,240p'",
      "original": "     1\t\"\"\"\n     2\tRidge regression\n     3\t\"\"\"\n     4\t\n     5\t# Author: Mathieu Blondel <mathieu@mblondel.org>\n     6\t#         Reuben Fletcher-Costin <reuben.fletchercostin@gmail.com>\n     7\t#         Fabian Pedregosa <fabian@fseoane.net>\n     8\t#         Michael Eickenberg <michael.eickenberg@nsup.org>\n     9\t# License: BSD 3 clause\n    10\t\n    11\t\n    12\tfrom abc import ABCMeta, abstractmethod\n    13\timport warnings\n    14\t\n    15\timport numpy as np\n    16\tfrom scipy import linalg\n    17\tfrom scipy import sparse\n    18\tfrom scipy.sparse import linalg as sp_linalg\n    19\t\n    20\tfrom .base import LinearClassifierMixin, LinearModel, _rescale_data\n    21\tfrom .sag import sag_solver\n    22\tfrom ..base import RegressorMixin\n    23\tfrom ..utils.extmath import safe_sparse_dot\n    24\tfrom ..utils.extmath import row_norms\n    25\tfrom ..utils import check_X_y\n    26\tfrom ..utils import check_array\n    27\tfrom ..utils import check_consistent_length\n    28\tfrom ..utils import compute_sample_weight\n    29\tfrom ..utils import column_or_1d\n    30\tfrom ..preprocessing import LabelBinarizer\n    31\tfrom ..model_selection import GridSearchCV\n    32\tfrom ..externals import six\n    33\tfrom ..metrics.scorer import check_scoring\n    34\t\n    35\t\n    36\tdef _solve_sparse_cg(X, y, alpha, max_iter=None, tol=1e-3, verbose=0):\n    37\t    n_samples, n_features = X.shape\n    38\t    X1 = sp_linalg.aslinearoperator(X)\n    39\t    coefs = np.empty((y.shape[1], n_features), dtype=X.dtype)\n    40\t\n    41\t    if n_features > n_samples:\n    42\t        def create_mv(curr_alpha):\n    43\t            def _mv(x):\n    44\t                return X1.matvec(X1.rmatvec(x)) + curr_alpha * x\n    45\t            return _mv\n    46\t    else:\n    47\t        def create_mv(curr_alpha):\n    48\t            def _mv(x):\n    49\t                return X1.rmatvec(X1.matvec(x)) + curr_alpha * x\n    50\t            return _mv\n    51\t\n    52\t    for i in range(y.shape[1]):\n    53\t        y_column = y[:, i]\n    54\t\n    55\t        mv = create_mv(alpha[i])\n    56\t        if n_features > n_samples:\n    57\t            # kernel ridge\n    58\t            # w = X.T * inv(X X^t + alpha*Id) y\n    59\t            C = sp_linalg.LinearOperator(\n    60\t                (n_samples, n_samples), matvec=mv, dtype=X.dtype)\n    61\t            coef, info = sp_linalg.cg(C, y_column, tol=tol)\n    62\t            coefs[i] = X1.rmatvec(coef)\n    63\t        else:\n    64\t            # linear ridge\n    65\t            # w = inv(X^t X + alpha*Id) * X.T y\n    66\t            y_column = X1.rmatvec(y_column)\n    67\t            C = sp_linalg.LinearOperator(\n    68\t                (n_features, n_features), matvec=mv, dtype=X.dtype)\n    69\t            coefs[i], info = sp_linalg.cg(C, y_column, maxiter=max_iter,\n    70\t                                          tol=tol)\n    71\t        if info < 0:\n    72\t            raise ValueError(\"Failed with error code %d\" % info)\n    73\t\n    74\t        if max_iter is None and info > 0 and verbose:\n    75\t            warnings.warn(\"sparse_cg did not converge after %d iterations.\" %\n    76\t                          info)\n    77\t\n    78\t    return coefs\n    79\t\n    80\t\n    81\tdef _solve_lsqr(X, y, alpha, max_iter=None, tol=1e-3):\n    82\t    n_samples, n_features = X.shape\n    83\t    coefs = np.empty((y.shape[1], n_features), dtype=X.dtype)\n    84\t    n_iter = np.empty(y.shape[1], dtype=np.int32)\n    85\t\n    86\t    # According to the lsqr documentation, alpha = damp^2.\n    87\t    sqrt_alpha = np.sqrt(alpha)\n    88\t\n    89\t    for i in range(y.shape[1]):\n    90\t        y_column = y[:, i]\n    91\t        info = sp_linalg.lsqr(X, y_column, damp=sqrt_alpha[i],\n    92\t                              atol=tol, btol=tol, iter_lim=max_iter)\n    93\t        coefs[i] = info[0]\n    94\t        n_iter[i] = info[2]\n    95\t\n    96\t    return coefs, n_iter\n    97\t\n    98\t\n    99\tdef _solve_cholesky(X, y, alpha):\n   100\t    # w = inv(X^t X + alpha*Id) * X.T y\n   101\t    n_samples, n_features = X.shape\n   102\t    n_targets = y.shape[1]\n   103\t\n   104\t    A = safe_sparse_dot(X.T, X, dense_output=True)\n   105\t    Xy = safe_sparse_dot(X.T, y, dense_output=True)\n   106\t\n   107\t    one_alpha = np.array_equal(alpha, len(alpha) * [alpha[0]])\n   108\t\n   109\t    if one_alpha:\n   110\t        A.flat[::n_features + 1] += alpha[0]\n   111\t        return linalg.solve(A, Xy, sym_pos=True,\n   112\t                            overwrite_a=True).T\n   113\t    else:\n   114\t        coefs = np.empty([n_targets, n_features], dtype=X.dtype)\n   115\t        for coef, target, current_alpha in zip(coefs, Xy.T, alpha):\n   116\t            A.flat[::n_features + 1] += current_alpha\n   117\t            coef[:] = linalg.solve(A, target, sym_pos=True,\n   118\t                                   overwrite_a=False).ravel()\n   119\t            A.flat[::n_features + 1] -= current_alpha\n   120\t        return coefs\n   121\t\n   122\t\n   123\tdef _solve_cholesky_kernel(K, y, alpha, sample_weight=None, copy=False):\n   124\t    # dual_coef = inv(X X^t + alpha*Id) y\n   125\t    n_samples = K.shape[0]\n   126\t    n_targets = y.shape[1]\n   127\t\n   128\t    if copy:\n   129\t        K = K.copy()\n   130\t\n   131\t    alpha = np.atleast_1d(alpha)\n   132\t    one_alpha = (alpha == alpha[0]).all()\n   133\t    has_sw = isinstance(sample_weight, np.ndarray) \\\n   134\t        or sample_weight not in [1.0, None]\n   135\t\n   136\t    if has_sw:\n   137\t        # Unlike other solvers, we need to support sample_weight directly\n   138\t        # because K might be a pre-computed kernel.\n   139\t        sw = np.sqrt(np.atleast_1d(sample_weight))\n   140\t        y = y * sw[:, np.newaxis]\n   141\t        K *= np.outer(sw, sw)\n   142\t\n   143\t    if one_alpha:\n   144\t        # Only one penalty, we can solve multi-target problems in one time.\n   145\t        K.flat[::n_samples + 1] += alpha[0]\n   146\t\n   147\t        try:\n   148\t            # Note: we must use overwrite_a=False in order to be able to\n   149\t            #       use the fall-back solution below in case a LinAlgError\n   150\t            #       is raised\n   151\t            dual_coef = linalg.solve(K, y, sym_pos=True,\n   152\t                                     overwrite_a=False)\n   153\t        except np.linalg.LinAlgError:\n   154\t            warnings.warn(\"Singular matrix in solving dual problem. Using \"\n   155\t                          \"least-squares solution instead.\")\n   156\t            dual_coef = linalg.lstsq(K, y)[0]\n   157\t\n   158\t        # K is expensive to compute and store in memory so change it back in\n   159\t        # case it was user-given.\n   160\t        K.flat[::n_samples + 1] -= alpha[0]\n   161\t\n   162\t        if has_sw:\n   163\t            dual_coef *= sw[:, np.newaxis]\n   164\t\n   165\t        return dual_coef\n   166\t    else:\n   167\t        # One penalty per target. We need to solve each target separately.\n   168\t        dual_coefs = np.empty([n_targets, n_samples], K.dtype)\n   169\t\n   170\t        for dual_coef, target, current_alpha in zip(dual_coefs, y.T, alpha):\n   171\t            K.flat[::n_samples + 1] += current_alpha\n   172\t\n   173\t            dual_coef[:] = linalg.solve(K, target, sym_pos=True,\n   174\t                                        overwrite_a=False).ravel()\n   175\t\n   176\t            K.flat[::n_samples + 1] -= current_alpha\n   177\t\n   178\t        if has_sw:\n   179\t            dual_coefs *= sw[np.newaxis, :]\n   180\t\n   181\t        return dual_coefs.T\n   182\t\n   183\t\n   184\tdef _solve_svd(X, y, alpha):\n   185\t    U, s, Vt = linalg.svd(X, full_matrices=False)\n   186\t    idx = s > 1e-15  # same default value as scipy.linalg.pinv\n   187\t    s_nnz = s[idx][:, np.newaxis]\n   188\t    UTy = np.dot(U.T, y)\n   189\t    d = np.zeros((s.size, alpha.size), dtype=X.dtype)\n   190\t    d[idx] = s_nnz / (s_nnz ** 2 + alpha)\n   191\t    d_UT_y = d * UTy\n   192\t    return np.dot(Vt.T, d_UT_y).T\n   193\t\n   194\t\n   195\tdef ridge_regression(X, y, alpha, sample_weight=None, solver='auto',\n   196\t                     max_iter=None, tol=1e-3, verbose=0, random_state=None,\n   197\t                     return_n_iter=False, return_intercept=False):\n   198\t    \"\"\"Solve the ridge equation by the method of normal equations.\n   199\t\n   200\t    Read more in the :ref:`User Guide <ridge_regression>`.\n   201\t\n   202\t    Parameters\n   203\t    ----------\n   204\t    X : {array-like, sparse matrix, LinearOperator},\n   205\t        shape = [n_samples, n_features]\n   206\t        Training data\n   207\t\n   208\t    y : array-like, shape = [n_samples] or [n_samples, n_targets]\n   209\t        Target values\n   210\t\n   211\t    alpha : {float, array-like},\n   212\t        shape = [n_targets] if array-like\n   213\t        Regularization strength; must be a positive float. Regularization\n   214\t        improves the conditioning of the problem and reduces the variance of\n   215\t        the estimates. Larger values specify stronger regularization.\n   216\t        Alpha corresponds to ``C^-1`` in other linear models such as\n   217\t        LogisticRegression or LinearSVC. If an array is passed, penalties are\n   218\t        assumed to be specific to the targets. Hence they must correspond in\n   219\t        number.\n   220\t\n   221\t    sample_weight : float or numpy array of shape [n_samples]\n   222\t        Individual weights for each sample. If sample_weight is not None and\n   223\t        solver='auto', the solver will be set to 'cholesky'.\n   224\t\n   225\t        .. versionadded:: 0.17\n   226\t\n   227\t    solver : {'auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga'}\n   228\t        Solver to use in the computational routines:\n   229\t\n   230\t        - 'auto' chooses the solver automatically based on the type of data.\n   231\t\n   232\t        - 'svd' uses a Singular Value Decomposition of X to compute the Ridge\n   233\t          coefficients. More stable for singular matrices than\n   234\t          'cholesky'.\n   235\t\n   236\t        - 'cholesky' uses the standard scipy.linalg.solve function to\n   237\t          obtain a closed-form solution via a Cholesky decomposition of\n   238\t          dot(X.T, X)\n   239\t\n   240\t        - 'sparse_cg' uses the conjugate gradient solver as found in\n",
      "perturbed": "2\tRidge regression\n3\t\"\"\"\n5\t# Author: Mathieu Blondel <mathieu@mblondel.org>\n4\n7\t#         Fabian Pedregosa <fabian@fseoane.net>\n8\t#         Michael Eickenberg <...\n9\t# License: BSD 3 clause\n10\n11\n13\timport warnings\n14\n15\timport numpy as np\n16\tfrom scipy import linalg\n18\tfrom scipy.sparse import linalg as sp_linalg\n20\tfrom .base imp...\n22\tfrom ..base import RegressorMixin\n23\tfrom ..utils.extmath import safe_sparse_dot\n24\tfrom ..utils.extmath import row_norms\n28\tfrom ..utils import compute_sample_weight\n25\tfrom ..utils import check_X_y\n29\tfrom ..utils import column_or_1d\n30\tfrom ..preprocessing import LabelBinarizer\n31\tfrom ..model_selection import GridSearchCV\n32\tfrom ..externals import six\n33\tfrom ..metrics.scorer import check_scoring\n38\t    X1 = sp_lin...\n39\t    coefs = np.empty((y.shape[1]...\n40\n41\t    if n_features > n_samples:\n43\t            def _mv(x):\n44\t                return X1.matvec(X1.rmatvec(x)) + curr_alpha * x\n45\t            return _mv\n47\t        def create_mv(curr_alpha):\n48\t            def _mv(x):\n49\t                return X1.rmatvec(X1.matvec(x)) + curr_alpha * x\n50\t            return _mv\n51\n54\n52\t    for i in range(y.shape[1]):\n55\t        mv = create_mv(alpha[i])\n57\t            # kernel ridge\n58\t            # w = X.T * inv(X X^t + alpha*Id) y\n59\t            C = sp_linalg.LinearOperator(\n62\t            coefs[i] = X1.rmatvec(coef)\n63\t        else:\n64\t            # linear ridge\n65\t            # w = inv(X^t X + alpha*Id) * X.T y\n67\t            C = sp_linalg.LinearOperator(\n66\t            y_column = X1.rmatvec(y_column)\n68\t                (n_features, n_features), matvec=mv, dtype=X.dtype)\n69\t            coefs[i], info = sp_linalg.cg(C, y_column, maxiter=max_iter,\n70\t             ...\n71\t        if info < 0:\n72\t            raise ValueError(\"Failed with error code %d\" %...\n74\t        if max_iter is None and info > 0 and verbose:\n75\t            warnings.warn(\"sparse_cg did not converge after %d iterations.\" %\n77\n79\n80\n81\tdef _solve_lsqr(X, y, alpha, max_iter=None, tol=1e-3):\n82\t    n_samples, n_features = X.shape\n84\t    n_iter = n...\n86\t    # According to the lsqr documentation, alpha = damp^2.\n85\n89\t    for i in range(y.shape[1]):\n87\t    sqrt_alpha = np.sqrt(alpha)\n90\t        y_column = y[:, i]\n91\t        info = sp_linalg.lsqr(X, y_column, damp=sqrt_alpha[i],\n94\t        n_iter[i] = info[2]\n95\n97\n98\n99\tdef _solve_cholesky(X, y, alpha):\n100\t    # w = inv(X^t X + alpha*Id) * X.T y\n104\t    A = safe_sparse_dot(X.T, X, dense_output=True)\n101\t    n_samples, n_features = X.shape\n105\t    Xy = safe_sparse_dot(X.T, y, dense_output=True)\n107\t    one_alpha = np.array_equal(alpha, len(alpha) * [alpha[0]])\n108\n109\t    if one_alpha:\n110\t        A.flat[::n_features + 1] += alpha[0]\n111\t        return linalg.solve(A, Xy, sym_pos=True,\n113\t    else:\n112\t                            overwrite_a=True).T\n115\t        for coef, target, current_alpha in zip(coefs, Xy.T, alpha):\n116\t            A.flat[::n_features + 1] += current_alpha\n117\t            coef[:] = linalg.solve(A, target, sym_pos=True,\n119\t            A.flat[::n_features + 1] -= current_alpha\n118\t                                   overwrite_a=False).ravel()\n123\tdef _solve_cholesky_kernel(K, y, alpha, sample_weight=None, copy=False):\n121\n126\t    n_targets = y.shape[1]\n129\t        K = K.copy()\n128\t    if copy:\n132\t    one_alpha = (alpha == alpha[0]).all()\n134\t        or sample_weight not in [1.0, None]\n135\n136\t    if has_sw:\n137\t        # Unlike other solvers, we need to support sample_weight directly\n138\t        # because K might be ...\n140\t        y = y * sw[:, np.newaxis]\n141\t        K *= np.outer(sw, sw)\n142\n144\t        # Only one penalty, we can solve multi-target problems in one time.\n146\n147\t        try:\n148\t            # Note: we must use overwrite_a=False in order to be able to\n150\t            #       is raised\n149\t            #       use the fall-back solution below in case a LinAlgError\n151\t            dual_coef = linalg.solve(K, y, sym_pos=True,\n153\t        except np.linalg.LinAlgError:\n154\t            warnings.warn(\"Singular matrix in solving dual problem. Using \"\n155\t                          \"least-squares solution instead.\")\n156\t            dual_coef = linalg.lstsq(K, y)[0]\n158\t        # K is expensive to compute and store in memory so change it back in\n160\t        K.flat[::n_samples + 1] -= alpha[0]\n162\t        if has_sw:\n161\n163\t            dual_coef *= sw[:, np.ne...\n168\t        dual_coefs = np.empty([n_targets, n_samples], K.dtype)\n167\t        # One penalty per target. We need to solve each target separately.\n169\n170\t        for dual_coef, target, current_alpha in zip(dual_coefs, y.T, alpha):\n173\t            dual_coef[:] = linalg.solve(K, target, sym_pos=True,\n171\t            K.flat[::n_samples + 1] += current_alpha\n176\t            K.flat[::n_samples + 1] -= current_alpha\n174\t                                        overwrite_a=False).ravel()\n179\t            dual_coefs *= sw[np.newaxis, :]\n180\n181\t        return dual_coefs.T\n182\n183\n184\tdef _solve_svd(X, y, alpha):\n185\t    U, s, Vt = linalg.svd(X, full_matrices=False)\n186\t    idx = s > 1e-15  # same de...\n187\t    s_nnz = s[idx][:, np.newaxis]\n188\t    UTy = np.dot(U.T, y)\n190\t    d[idx] = s_nnz / (s_nnz ** 2 + alpha)\n191\t    d_UT_y = d * UTy\n194\n195\tdef ridge_regression(X, y, alpha, sample_weight=None, solver='auto',\n196\t                     max_iter=None, tol=1e-3, verbose=0, random_state=None,\n197\t                     return_n_iter=False, return_i...\n198\t    \"\"\"Solve the ridge equation by the method of normal equations.\n200\t    Read more in the :ref:`User Guide <ridge_regression>`.\n199\n201\n203\t    ----------\n202\t    Parameters\n204\t    X : {array-like, sparse matrix, LinearOperator},\n205\t        shape = [n_samples, n_features]\n208\t    y : array-like, shape = [n_samples] or [n_samples, n_targets]\n207\n209\t        Target values\n212\t        shape = [n_targets] if array-like\n214\t        improves the conditioning of the problem and reduces the variance of\n215\t        the estimates. Larger values specify s...\n216\t        Alpha corresponds to ``C^-1`` in other linear models such as\n217\t        LogisticRegression or LinearSVC. If an array is passed, penalties are\n218\t        assumed to be specific to the targets. Hence they must correspond in\n219\t        number.\n220\n221\t    sample_weight : float or numpy array of shape [n_samples]\n222\t        Individual weights for each sample. If sample_weight is not None and\n224\n225\t        .. versionadded:: 0.17\n226\n227\t    solver : {'auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga'}\n229\n228\t        Solver to use in the computational routines:\n231\n230\t        - 'auto' chooses t...\n232\t        - 'svd' uses a Singular Value Decomposition of X to compute the Ridge\n234\t          'cholesky'.\n233\t          coefficients....\n235\n236\t        - 'cholesky' uses the standard scipy.linalg.solve function to\n237\t          obtain a closed-form solution via a Cholesky decomposition of\n238\t          dot(X.T, X)\n239",
      "applied_operators": [
        "text_perturbation"
      ]
    },
    {
      "action": "nl -ba sklearn/linear_model/ridge.py | sed -n '240,520p'",
      "original": "   240\t        - 'sparse_cg' uses the conjugate gradient solver as found in\n   241\t          scipy.sparse.linalg.cg. As an iterative algorithm, this solver is\n   242\t          more appropriate than 'cholesky' for large-scale data\n   243\t          (possibility to set `tol` and `max_iter`).\n   244\t\n   245\t        - 'lsqr' uses the dedicated regularized least-squares routine\n   246\t          scipy.sparse.linalg.lsqr. It is the fastest but may not be available\n   247\t          in old scipy versions. It also uses an iterative procedure.\n   248\t\n   249\t        - 'sag' uses a Stochastic Average Gradient descent, and 'saga' uses\n   250\t          its improved, unbiased version named SAGA. Both methods also use an\n   251\t          iterative procedure, and are often faster than other solvers when\n   252\t          both n_samples and n_features are large. Note that 'sag' and\n   253\t          'saga' fast convergence is only guaranteed on features with\n   254\t          approximately the same scale. You can preprocess the data with a\n   255\t          scaler from sklearn.preprocessing.\n   256\t\n   257\t\n   258\t        All last five solvers support both dense and sparse data. However, only\n   259\t        'sag' and 'saga' supports sparse input when`fit_intercept` is True.\n   260\t\n   261\t        .. versionadded:: 0.17\n   262\t           Stochastic Average Gradient descent solver.\n   263\t        .. versionadded:: 0.19\n   264\t           SAGA solver.\n   265\t\n   266\t    max_iter : int, optional\n   267\t        Maximum number of iterations for conjugate gradient solver.\n   268\t        For the 'sparse_cg' and 'lsqr' solvers, the default value is determined\n   269\t        by scipy.sparse.linalg. For 'sag' and saga solver, the default value is\n   270\t        1000.\n   271\t\n   272\t    tol : float\n   273\t        Precision of the solution.\n   274\t\n   275\t    verbose : int\n   276\t        Verbosity level. Setting verbose > 0 will display additional\n   277\t        information depending on the solver used.\n   278\t\n   279\t    random_state : int, RandomState instance or None, optional, default None\n   280\t        The seed of the pseudo random number generator to use when shuffling\n   281\t        the data.  If int, random_state is the seed used by the random number\n   282\t        generator; If RandomState instance, random_state is the random number\n   283\t        generator; If None, the random number generator is the RandomState\n   284\t        instance used by `np.random`. Used when ``solver`` == 'sag'.\n   285\t\n   286\t    return_n_iter : boolean, default False\n   287\t        If True, the method also returns `n_iter`, the actual number of\n   288\t        iteration performed by the solver.\n   289\t\n   290\t        .. versionadded:: 0.17\n   291\t\n   292\t    return_intercept : boolean, default False\n   293\t        If True and if X is sparse, the method also returns the intercept,\n   294\t        and the solver is automatically changed to 'sag'. This is only a\n   295\t        temporary fix for fitting the intercept with sparse data. For dense\n   296\t        data, use sklearn.linear_model._preprocess_data before your regression.\n   297\t\n   298\t        .. versionadded:: 0.17\n   299\t\n   300\t    Returns\n   301\t    -------\n   302\t    coef : array, shape = [n_features] or [n_targets, n_features]\n   303\t        Weight vector(s).\n   304\t\n   305\t    n_iter : int, optional\n   306\t        The actual number of iteration performed by the solver.\n   307\t        Only returned if `return_n_iter` is True.\n   308\t\n   309\t    intercept : float or array, shape = [n_targets]\n   310\t        The intercept of the model. Only returned if `return_intercept`\n   311\t        is True and if X is a scipy sparse array.\n   312\t\n   313\t    Notes\n   314\t    -----\n   315\t    This function won't compute the intercept.\n   316\t    \"\"\"\n   317\t    if return_intercept and sparse.issparse(X) and solver != 'sag':\n   318\t        if solver != 'auto':\n   319\t            warnings.warn(\"In Ridge, only 'sag' solver can currently fit the \"\n   320\t                          \"intercept when X is sparse. Solver has been \"\n   321\t                          \"automatically changed into 'sag'.\")\n   322\t        solver = 'sag'\n   323\t\n   324\t    _dtype = [np.float64, np.float32]\n   325\t\n   326\t    # SAG needs X and y columns to be C-contiguous and np.float64\n   327\t    if solver in ['sag', 'saga']:\n   328\t        X = check_array(X, accept_sparse=['csr'],\n   329\t                        dtype=np.float64, order='C')\n   330\t        y = check_array(y, dtype=np.float64, ensure_2d=False, order='F')\n   331\t    else:\n   332\t        X = check_array(X, accept_sparse=['csr', 'csc', 'coo'],\n   333\t                        dtype=_dtype)\n   334\t        y = check_array(y, dtype=X.dtype, ensure_2d=False)\n   335\t    check_consistent_length(X, y)\n   336\t\n   337\t    n_samples, n_features = X.shape\n   338\t\n   339\t    if y.ndim > 2:\n   340\t        raise ValueError(\"Target y has the wrong shape %s\" % str(y.shape))\n   341\t\n   342\t    ravel = False\n   343\t    if y.ndim == 1:\n   344\t        y = y.reshape(-1, 1)\n   345\t        ravel = True\n   346\t\n   347\t    n_samples_, n_targets = y.shape\n   348\t\n   349\t    if n_samples != n_samples_:\n   350\t        raise ValueError(\"Number of samples in X and y does not correspond:\"\n   351\t                         \" %d != %d\" % (n_samples, n_samples_))\n   352\t\n   353\t    has_sw = sample_weight is not None\n   354\t\n   355\t    if solver == 'auto':\n   356\t        # cholesky if it's a dense array and cg in any other case\n   357\t        if not sparse.issparse(X) or has_sw:\n   358\t            solver = 'cholesky'\n   359\t        else:\n   360\t            solver = 'sparse_cg'\n   361\t\n   362\t    elif solver == 'lsqr' and not hasattr(sp_linalg, 'lsqr'):\n   363\t        warnings.warn(\"\"\"lsqr not available on this machine, falling back\n   364\t                      to sparse_cg.\"\"\")\n   365\t        solver = 'sparse_cg'\n   366\t\n   367\t    if has_sw:\n   368\t        if np.atleast_1d(sample_weight).ndim > 1:\n   369\t            raise ValueError(\"Sample weights must be 1D array or scalar\")\n   370\t\n   371\t        if solver not in ['sag', 'saga']:\n   372\t            # SAG supports sample_weight directly. For other solvers,\n   373\t            # we implement sample_weight via a simple rescaling.\n   374\t            X, y = _rescale_data(X, y, sample_weight)\n   375\t\n   376\t    # There should be either 1 or n_targets penalties\n   377\t    alpha = np.asarray(alpha, dtype=X.dtype).ravel()\n   378\t    if alpha.size not in [1, n_targets]:\n   379\t        raise ValueError(\"Number of targets and number of penalties \"\n   380\t                         \"do not correspond: %d != %d\"\n   381\t                         % (alpha.size, n_targets))\n   382\t\n   383\t    if alpha.size == 1 and n_targets > 1:\n   384\t        alpha = np.repeat(alpha, n_targets)\n   385\t\n   386\t    if solver not in ('sparse_cg', 'cholesky', 'svd', 'lsqr', 'sag', 'saga'):\n   387\t        raise ValueError('Solver %s not understood' % solver)\n   388\t\n   389\t    n_iter = None\n   390\t    if solver == 'sparse_cg':\n   391\t        coef = _solve_sparse_cg(X, y, alpha, max_iter, tol, verbose)\n   392\t\n   393\t    elif solver == 'lsqr':\n   394\t        coef, n_iter = _solve_lsqr(X, y, alpha, max_iter, tol)\n   395\t\n   396\t    elif solver == 'cholesky':\n   397\t        if n_features > n_samples:\n   398\t            K = safe_sparse_dot(X, X.T, dense_output=True)\n   399\t            try:\n   400\t                dual_coef = _solve_cholesky_kernel(K, y, alpha)\n   401\t\n   402\t                coef = safe_sparse_dot(X.T, dual_coef, dense_output=True).T\n   403\t            except linalg.LinAlgError:\n   404\t                # use SVD solver if matrix is singular\n   405\t                solver = 'svd'\n   406\t\n   407\t        else:\n   408\t            try:\n   409\t                coef = _solve_cholesky(X, y, alpha)\n   410\t            except linalg.LinAlgError:\n   411\t                # use SVD solver if matrix is singular\n   412\t                solver = 'svd'\n   413\t\n   414\t    elif solver in ['sag', 'saga']:\n   415\t        # precompute max_squared_sum for all targets\n   416\t        max_squared_sum = row_norms(X, squared=True).max()\n   417\t\n   418\t        coef = np.empty((y.shape[1], n_features))\n   419\t        n_iter = np.empty(y.shape[1], dtype=np.int32)\n   420\t        intercept = np.zeros((y.shape[1], ))\n   421\t        for i, (alpha_i, target) in enumerate(zip(alpha, y.T)):\n   422\t            init = {'coef': np.zeros((n_features + int(return_intercept), 1))}\n   423\t            coef_, n_iter_, _ = sag_solver(\n   424\t                X, target.ravel(), sample_weight, 'squared', alpha_i, 0,\n   425\t                max_iter, tol, verbose, random_state, False, max_squared_sum,\n   426\t                init,\n   427\t                is_saga=solver == 'saga')\n   428\t            if return_intercept:\n   429\t                coef[i] = coef_[:-1]\n   430\t                intercept[i] = coef_[-1]\n   431\t            else:\n   432\t                coef[i] = coef_\n   433\t            n_iter[i] = n_iter_\n   434\t\n   435\t        if intercept.shape[0] == 1:\n   436\t            intercept = intercept[0]\n   437\t        coef = np.asarray(coef)\n   438\t\n   439\t    if solver == 'svd':\n   440\t        if sparse.issparse(X):\n   441\t            raise TypeError('SVD solver does not support sparse'\n   442\t                            ' inputs currently')\n   443\t        coef = _solve_svd(X, y, alpha)\n   444\t\n   445\t    if ravel:\n   446\t        # When y was passed as a 1d-array, we flatten the coefficients.\n   447\t        coef = coef.ravel()\n   448\t\n   449\t    if return_n_iter and return_intercept:\n   450\t        return coef, n_iter, intercept\n   451\t    elif return_intercept:\n   452\t        return coef, intercept\n   453\t    elif return_n_iter:\n   454\t        return coef, n_iter\n   455\t    else:\n   456\t        return coef\n   457\t\n   458\t\n   459\tclass _BaseRidge(six.with_metaclass(ABCMeta, LinearModel)):\n   460\t\n   461\t    @abstractmethod\n   462\t    def __init__(self, alpha=1.0, fit_intercept=True, normalize=False,\n   463\t                 copy_X=True, max_iter=None, tol=1e-3, solver=\"auto\",\n   464\t                 random_state=None):\n   465\t        self.alpha = alpha\n   466\t        self.fit_intercept = fit_intercept\n   467\t        self.normalize = normalize\n   468\t        self.copy_X = copy_X\n   469\t        self.max_iter = max_iter\n   470\t        self.tol = tol\n   471\t        self.solver = solver\n   472\t        self.random_state = random_state\n   473\t\n   474\t    def fit(self, X, y, sample_weight=None):\n   475\t\n   476\t        if self.solver in ('sag', 'saga'):\n   477\t            _dtype = np.float64\n   478\t        else:\n   479\t            # all other solvers work at both float precision levels\n   480\t            _dtype = [np.float64, np.float32]\n   481\t\n   482\t        X, y = check_X_y(X, y, ['csr', 'csc', 'coo'], dtype=_dtype,\n   483\t                         multi_output=True, y_numeric=True)\n   484\t\n   485\t        if ((sample_weight is not None) and\n   486\t                np.atleast_1d(sample_weight).ndim > 1):\n   487\t            raise ValueError(\"Sample weights must be 1D array or scalar\")\n   488\t\n   489\t        X, y, X_offset, y_offset, X_scale = self._preprocess_data(\n   490\t            X, y, self.fit_intercept, self.normalize, self.copy_X,\n   491\t            sample_weight=sample_weight)\n   492\t\n   493\t        # temporary fix for fitting the intercept with sparse data using 'sag'\n   494\t        if sparse.issparse(X) and self.fit_intercept:\n   495\t            self.coef_, self.n_iter_, self.intercept_ = ridge_regression(\n   496\t                X, y, alpha=self.alpha, sample_weight=sample_weight,\n   497\t                max_iter=self.max_iter, tol=self.tol, solver=self.solver,\n   498\t                random_state=self.random_state, return_n_iter=True,\n   499\t                return_intercept=True)\n   500\t            self.intercept_ += y_offset\n   501\t        else:\n   502\t            self.coef_, self.n_iter_ = ridge_regression(\n   503\t                X, y, alpha=self.alpha, sample_weight=sample_weight,\n   504\t                max_iter=self.max_iter, tol=self.tol, solver=self.solver,\n   505\t                random_state=self.random_state, return_n_iter=True,\n   506\t                return_intercept=False)\n   507\t            self._set_intercept(X_offset, y_offset, X_scale)\n   508\t\n   509\t        return self\n   510\t\n   511\t\n   512\tclass Ridge(_BaseRidge, RegressorMixin):\n   513\t    \"\"\"Linear least squares with l2 regularization.\n   514\t\n   515\t    Minimizes the objective function::\n   516\t\n   517\t    ||y - Xw||^2_2 + alpha * ||w||^2_2\n   518\t\n   519\t    This model solves a regression model where the loss function is\n   520\t    the linear least squares function and regularization is given by\n",
      "perturbed": "241\t          scipy.sparse.linalg.cg. As an iterative algorithm, this solver is\n242\t          more appropriate than 'cholesky' for large-scale data\n245\t        - 'lsqr' uses the dedicated regularized least-squares routine\n246\t          scipy.sparse.linalg.lsqr. It is the fastest but may not be available\n248\n249\t        - 'sag' uses a Stochastic Average Gradient descent, and 'saga' uses\n250\t          its improved, unbiased version named SAGA. Both methods also use an\n252\t          both n_samples and n_features are large. Note tha...\n251\t          iterative procedure, and are often faster than other solvers when\n253\t          'saga' f...\n254\t          approximately the same scale. You can preprocess the data with a\n256\n255\t          scaler from sklearn.preprocessing.\n257\n258\t        All last five solvers support both dense and sparse data. However, only\n259\t        'sag' and 'saga' supports sparse input when`fit_intercept` is...\n262\t           Stochastic Average Gradient descent solver.\n260\n263\t        .. versionadded:: 0.19\n264\t           SAGA solver.\n266\t    max_iter : int, optional\n267\t        Maximum number of iterations for conjugate gradient solver.\n268\t        For the 'sparse_cg' and 'lsqr' solvers, the default value is determined\n269\t        by scipy.sparse.linalg. For 'sag' and saga solver, the default value is\n270\t        1000.\n271\n272\t    tol : float\n273\t        Precision of the solution.\n274\n275\t    verbose : int\n276\t        Verbosity level. Setting verbose > 0 will display additional\n277\t        information depen...\n278\n280\t        The seed of the pseudo random number generator to use when shuffling\n281\t        the data.  If int, random_state is the seed used by the random number\n282\t        generator; If RandomState instance, random_state is the random number\n283\t        generator; If None, the random number generator is the RandomState\n285\n284\t        instance used ...\n286\t    return_n_iter : boolean, default ...\n287\t        If True, the method also returns `n_iter`, the actual number of\n288\t        iteration performed by the solver.\n289\n290\t        .. versionadded:: 0.17\n291\n293\t        If True and if X is sparse, the method also returns the intercept,\n292\t    return_intercept : boolean, default False\n294\t        and the solver is automatically changed to 'sag'. This is only a\n296\t        data, use sklearn.linear_model._preprocess_data before your regression.\n295\t        temporary fix for fitting the intercept with sparse data. For dense\n299\n301\t    -------\n303\t        Weight vector(s).\n302\t    coef : array, shape = [n_features] or [n_targets, n_features]\n304\n305\t    n_iter : int, optional\n308\n309\t    intercept : float or array, shape = [n_targets]\n310\t        The intercept of the model. Only returned if `return_intercept`\n313\t    Notes\n314\t    -----\n315\t    This function won't compute the intercept.\n316\t    \"\"\"\n317\t    if return_intercept and sparse.issparse(X) and solver != 'sag':\n319\t            war...\n320\t                          \"intercept when ...\n322\t        solver = 'sag'\n321\t                          \"automatically changed into 'sag'.\")\n325\n323\n326\t    # SAG needs X and y columns to be C-contiguous and np.float64\n330\t        y = check_array(y, dtype=np.float64, ensure_2d=False,...\n331\t    else:\n332\t        X = check_array(X, accept_sparse=['csr', 'csc', 'coo'],\n333\t                        dtype=_dtype)\n334\t        y = check_array(y, dtype=X.dtype, ensure_2d=False)\n335\t    check_consistent_length(X, y)\n336\n337\t    n_samples, n_features = X.shape\n339\t    if y.ndim > 2:\n338\n340\t        raise ValueError(\"Target y has the wrong shape %s\" % str(y.shape))\n343\t    if y.ndim == 1:\n344\t        y = y.reshape(-1, 1)\n345\t        ravel = True\n348\n349\t    if n_samples != n_samples_:\n352\n351\t                         \" %d != %d\" % (n_samples, n_samples_))\n354\n356\t        # cholesky if it's a dense array and cg in any other case\n355\t    if solver == 'auto':\n357\t        if not sparse.issparse(X) or has_sw:\n358\t            solver = 'cholesky'\n359\t        else:\n360\t            solver = 'sparse_cg'\n362\t    elif solver == 'lsqr' and not hasattr(sp_linalg, 'lsqr'):\n361\n364\t                      to sparse_cg.\"\"\")\n363\t        warnings.warn(\"\"\"lsqr not available on this machine, falling back\n367\t    if has_sw:\n368\t        if np.atleast_1d(sample_weight).ndim > 1:\n369\t            raise ValueError(\"Sample weights must be 1D array or scalar\")\n371\t        if solver not in ['sag', 'saga']:\n370\n372\t            # SAG supports sample_weight directly. For other solvers,\n373\t            # we implement sample_weight via a simple rescaling.\n374\t            X, y = _rescale_data(X, y, sample_weight)\n376\t    # There should be either 1 or n_targets penalties\n375\n377\t    alpha = np.asarray(alpha, dtype=X.dtype).ravel()\n378\t    if alpha.size not in [1, n_targets]:\n379\t        raise ValueError(\"Number of targets and number of penalties \"\n383\t    if alpha.size == 1 and n_targets > 1:\n380\t                         \"do not correspond: %d != %d\"\n384\t        alpha = np.repeat(alpha, n_targets)\n387\t        raise ValueError('Solver %s not understood' % solver)\n390\t    if solver == 'sparse_cg':\n389\t    n_iter = None\n392\n391\t        coef = _solve_sparse_cg(X, y, alpha, max_iter, tol, verbose)\n393\t    elif solver == 'lsqr':\n394\t        coef, n_iter = _solve_lsqr(X, y, alpha, max_iter, tol)\n397\t        if n_featur...\n396\t    elif solver == 'cholesky':\n398\t            K = safe_sparse_dot(X,...\n401\n402\t                coef = safe_sparse_dot(X.T, dual_coef, dense_output=True).T\n403\t            except linalg.LinAlgError:\n406\n408\t            try:\n407\t        else:\n409\t                coef = _solve_cholesky(X, y, alpha)\n410\t            except linalg.LinAlgError:\n411\t                # use SVD solver if matrix is singular\n414\t    elif solver in ...\n412\t                solver = 'svd'\n415\t        # precompute max_squared_sum for all targets\n418\t        coef = np.empty((y.shape[1], n_features))\n417\n419\t        n_iter = np.empty(y.shape[1], dtype=np.int32)\n421\t        for i, (alpha_i, target) in enumerate(zip(alpha, y.T)):\n422\t            init = {'coef': np.zeros((n_features + int(return_intercept), 1))}\n424\t                X, target.ravel(), sample_weight, 'squared', alpha_i, 0,\n425\t                max_iter, tol, verbose, random_state, False, max_squared_sum,\n426\t                init,\n428\t            if return_intercept:\n430\t                intercept[i] = coef_[-1]\n429\t                coef[i] = coef_[:-1]\n432\t                coef[i] = coef_\n434\n436\t            intercept = intercept[0]\n439\t    if solver == 'svd':\n437\t        coef = np.asarray(coef)\n441\t            raise TypeError('SVD solver does not support sparse'\n443\t        coef = _solve_svd(X, y, alpha)\n444\n445\t    if ravel:\n447\t        coef = coef.ravel()\n446\t        # When y was passed as a 1d-array, we flatten the coefficients.\n448\n449\t    if return_n_iter and return_intercept:\n450\t        return coef, n_iter, intercept\n453\t    elif return_n_iter:\n454\t        return coef, n_iter\n455\t    else:\n460\n458\n461\t    @abstractmethod\n463\t                 copy_X=True, max_iter=None, tol=1e-3, solver=\"auto\",\n464\t                 random_state=None):\n465\t        self.alpha = alpha\n467\t        self.normalize = normalize\n469\t        self.max_iter = max_iter\n470\t        self.tol = tol\n472\t        self.random_state = random_state\n475\n474\t    def fit(self, X, y, sample_weight=None):\n478\t        else:\n477\t            _dtype = np.float64\n479\t            # all other solvers work at both float precision levels\n480\t            _dtype = [np.float...\n481\n485\t        if ((sample_weight is not None) and\n482\t        X, y = check_X_y(X, y, ['csr', 'csc', 'coo'], dtype=_dtype,\n486\t                np.atleast_1d(sample_weight).ndim > 1):\n487\t            raise ValueError(\"Sample weights must be 1D array or scalar\")\n488\n489\t        X, y, X_offset, y_offset, X_scale = self._preprocess_data(\n490\t            X, y, self.fit_intercept, self.normalize, self...\n492\n491\t            sample_weight=sample_weight)\n493\t        # temporary fix for fitting the intercept with sparse data using 'sag'\n495\t            self.coef_, self.n_iter_, self.intercept_ = ridge_regression(\n501\t        else:\n496\t                X, y, alpha=self.alpha, sample_weight=sample_weight,\n502\t            self.coef_, self.n_iter_ = ridge_reg...\n503\t                X, y, alpha=self.alpha, sample_weight=sample_weight,\n506\t                return_intercept=False)\n504\t                max_iter=self.max_iter, tol=self.tol, solver=self.solver,\n508\n507\t            self._set_intercept(X_offset, y_offset, X_scale)\n510\n511\n512\tclass Ridge(_BaseRidge, RegressorMixin):\n515\t    Minimizes the objective function::\n514\n516\n517\t    ||y - Xw||^2_2 + alpha * ||w||^2_2\n518\n519\t    This model solves a regression model where the loss function is\n520\t    the linear least squares function and regularization is given by",
      "applied_operators": [
        "text_perturbation"
      ]
    },
    {
      "action": "grep -R \"RidgeClassifierCV\" -n",
      "original": "doc/modules/multiclass.rst:87:  - :class:`sklearn.linear_model.RidgeClassifierCV`\ndoc/modules/multiclass.rst:118:  - :class:`sklearn.linear_model.RidgeClassifierCV`\ndoc/modules/classes.rst:697:   linear_model.RidgeClassifierCV\ndoc/modules/grid_search.rst:263:   linear_model.RidgeClassifierCV\nsklearn/linear_model/tests/test_ridge.py:28:from sklearn.linear_model.ridge import RidgeClassifierCV\nsklearn/linear_model/tests/test_ridge.py:437:    for reg in (RidgeClassifier(), RidgeClassifierCV()):\nsklearn/linear_model/tests/test_ridge.py:444:    reg = RidgeClassifierCV(cv=cv)\nsklearn/linear_model/tests/test_ridge.py:534:    for reg in (RidgeClassifier, RidgeClassifierCV):\nsklearn/linear_model/tests/test_ridge.py:567:    reg = RidgeClassifierCV(class_weight=None, alphas=[.01, .1, 1])\nsklearn/linear_model/tests/test_ridge.py:571:    reg = RidgeClassifierCV(class_weight={1: 0.001}, alphas=[.01, .1, 1, 10])\nsklearn/linear_model/__init__.py:24:from .ridge import (Ridge, RidgeCV, RidgeClassifier, RidgeClassifierCV,\nsklearn/linear_model/__init__.py:73:           'RidgeClassifierCV',\nsklearn/linear_model/ridge.py:777:    RidgeClassifierCV :  Ridge classifier with built-in cross validation\nsklearn/linear_model/ridge.py:1242:    RidgeClassifierCV : Ridge classifier with built-in cross validation\nsklearn/linear_model/ridge.py:1247:class RidgeClassifierCV(LinearClassifierMixin, _BaseRidgeCV):\nsklearn/linear_model/ridge.py:1337:        super(RidgeClassifierCV, self).__init__(\n",
      "perturbed": "doc/modules/multiclass.rst:87:  - :class:`sklearn.linear_model.RidgeClassifierCV`\ndoc/modules/multiclass.rst:118:  - :class:`sklearn.linear_model.RidgeClassifierCV`\ndoc/modules/classes.rst:697:   linear_model.RidgeClassifierCV\nsklearn/linear_model/tests/test_ridg...\ndoc/modules/grid_search.rst:263:   linear_model.RidgeClassifierCV\nsklearn/linear_model/tests/test_ridge.py:444:    reg = RidgeClassifierCV(cv=cv)\nsklearn/linear_model/tests/test_ridge.py:534:    for reg in (RidgeClassifier, RidgeClassifierCV):\nsklearn/linear_model/tests/test_ridge.py:567:    reg = RidgeClassifierCV(class_weight=None, alphas=[.01, .1, 1])\nsklearn/linear_model/tests/test_ridge.py:571:    reg = RidgeClassifierCV(class_weight={1: 0.001}, alphas=[.01, .1, 1, 10])\nsklearn/linear_model/__init__.py:24:from .ridge import (Ridge, RidgeCV, RidgeClassifier, RidgeClassifierCV,\nsklearn/linear_model/ridge.py:1242:    RidgeClassifierCV : Ridge classifier with built-in cross validation\nsklearn/linear_model/ridge.py:1337:        super(RidgeClassifierCV, self).__init__(\nsklearn/linear_model/ridge.py:1247:class RidgeClassifierCV(LinearClassifierMixin, _BaseRidgeCV):",
      "applied_operators": [
        "text_perturbation"
      ]
    },
    {
      "action": "nl -ba sklearn/linear_model/ridge.py | sed -n '1240,1400p'",
      "original": "  1240\t    Ridge : Ridge regression\n  1241\t    RidgeClassifier : Ridge classifier\n  1242\t    RidgeClassifierCV : Ridge classifier with built-in cross validation\n  1243\t    \"\"\"\n  1244\t    pass\n  1245\t\n  1246\t\n  1247\tclass RidgeClassifierCV(LinearClassifierMixin, _BaseRidgeCV):\n  1248\t    \"\"\"Ridge classifier with built-in cross-validation.\n  1249\t\n  1250\t    By default, it performs Generalized Cross-Validation, which is a form of\n  1251\t    efficient Leave-One-Out cross-validation. Currently, only the n_features >\n  1252\t    n_samples case is handled efficiently.\n  1253\t\n  1254\t    Read more in the :ref:`User Guide <ridge_regression>`.\n  1255\t\n  1256\t    Parameters\n  1257\t    ----------\n  1258\t    alphas : numpy array of shape [n_alphas]\n  1259\t        Array of alpha values to try.\n  1260\t        Regularization strength; must be a positive float. Regularization\n  1261\t        improves the conditioning of the problem and reduces the variance of\n  1262\t        the estimates. Larger values specify stronger regularization.\n  1263\t        Alpha corresponds to ``C^-1`` in other linear models such as\n  1264\t        LogisticRegression or LinearSVC.\n  1265\t\n  1266\t    fit_intercept : boolean\n  1267\t        Whether to calculate the intercept for this model. If set\n  1268\t        to false, no intercept will be used in calculations\n  1269\t        (e.g. data is expected to be already centered).\n  1270\t\n  1271\t    normalize : boolean, optional, default False\n  1272\t        This parameter is ignored when ``fit_intercept`` is set to False.\n  1273\t        If True, the regressors X will be normalized before regression by\n  1274\t        subtracting the mean and dividing by the l2-norm.\n  1275\t        If you wish to standardize, please use\n  1276\t        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\n  1277\t        on an estimator with ``normalize=False``.\n  1278\t\n  1279\t    scoring : string, callable or None, optional, default: None\n  1280\t        A string (see model evaluation documentation) or\n  1281\t        a scorer callable object / function with signature\n  1282\t        ``scorer(estimator, X, y)``.\n  1283\t\n  1284\t    cv : int, cross-validation generator or an iterable, optional\n  1285\t        Determines the cross-validation splitting strategy.\n  1286\t        Possible inputs for cv are:\n  1287\t\n  1288\t        - None, to use the efficient Leave-One-Out cross-validation\n  1289\t        - integer, to specify the number of folds.\n  1290\t        - An object to be used as a cross-validation generator.\n  1291\t        - An iterable yielding train/test splits.\n  1292\t\n  1293\t        Refer :ref:`User Guide <cross_validation>` for the various\n  1294\t        cross-validation strategies that can be used here.\n  1295\t\n  1296\t    class_weight : dict or 'balanced', optional\n  1297\t        Weights associated with classes in the form ``{class_label: weight}``.\n  1298\t        If not given, all classes are supposed to have weight one.\n  1299\t\n  1300\t        The \"balanced\" mode uses the values of y to automatically adjust\n  1301\t        weights inversely proportional to class frequencies in the input data\n  1302\t        as ``n_samples / (n_classes * np.bincount(y))``\n  1303\t\n  1304\t    Attributes\n  1305\t    ----------\n  1306\t    cv_values_ : array, shape = [n_samples, n_alphas] or \\\n  1307\t    shape = [n_samples, n_responses, n_alphas], optional\n  1308\t        Cross-validation values for each alpha (if `store_cv_values=True` and\n  1309\t    `cv=None`). After `fit()` has been called, this attribute will contain \\\n  1310\t    the mean squared errors (by default) or the values of the \\\n  1311\t    `{loss,score}_func` function (if provided in the constructor).\n  1312\t\n  1313\t    coef_ : array, shape = [n_features] or [n_targets, n_features]\n  1314\t        Weight vector(s).\n  1315\t\n  1316\t    intercept_ : float | array, shape = (n_targets,)\n  1317\t        Independent term in decision function. Set to 0.0 if\n  1318\t        ``fit_intercept = False``.\n  1319\t\n  1320\t    alpha_ : float\n  1321\t        Estimated regularization parameter\n  1322\t\n  1323\t    See also\n  1324\t    --------\n  1325\t    Ridge : Ridge regression\n  1326\t    RidgeClassifier : Ridge classifier\n  1327\t    RidgeCV : Ridge regression with built-in cross validation\n  1328\t\n  1329\t    Notes\n  1330\t    -----\n  1331\t    For multi-class classification, n_class classifiers are trained in\n  1332\t    a one-versus-all approach. Concretely, this is implemented by taking\n  1333\t    advantage of the multi-variate response support in Ridge.\n  1334\t    \"\"\"\n  1335\t    def __init__(self, alphas=(0.1, 1.0, 10.0), fit_intercept=True,\n  1336\t                 normalize=False, scoring=None, cv=None, class_weight=None):\n  1337\t        super(RidgeClassifierCV, self).__init__(\n  1338\t            alphas=alphas, fit_intercept=fit_intercept, normalize=normalize,\n  1339\t            scoring=scoring, cv=cv)\n  1340\t        self.class_weight = class_weight\n  1341\t\n  1342\t    def fit(self, X, y, sample_weight=None):\n  1343\t        \"\"\"Fit the ridge classifier.\n  1344\t\n  1345\t        Parameters\n  1346\t        ----------\n  1347\t        X : array-like, shape (n_samples, n_features)\n  1348\t            Training vectors, where n_samples is the number of samples\n  1349\t            and n_features is the number of features.\n  1350\t\n  1351\t        y : array-like, shape (n_samples,)\n  1352\t            Target values. Will be cast to X's dtype if necessary\n  1353\t\n  1354\t        sample_weight : float or numpy array of shape (n_samples,)\n  1355\t            Sample weight.\n  1356\t\n  1357\t        Returns\n  1358\t        -------\n  1359\t        self : object\n  1360\t        \"\"\"\n  1361\t        check_X_y(X, y, accept_sparse=['csr', 'csc', 'coo'],\n  1362\t                  multi_output=True)\n  1363\t\n  1364\t        self._label_binarizer = LabelBinarizer(pos_label=1, neg_label=-1)\n  1365\t        Y = self._label_binarizer.fit_transform(y)\n  1366\t        if not self._label_binarizer.y_type_.startswith('multilabel'):\n  1367\t            y = column_or_1d(y, warn=True)\n  1368\t\n  1369\t        if self.class_weight:\n  1370\t            if sample_weight is None:\n  1371\t                sample_weight = 1.\n  1372\t            # modify the sample weights with the corresponding class weight\n  1373\t            sample_weight = (sample_weight *\n  1374\t                             compute_sample_weight(self.class_weight, y))\n  1375\t\n  1376\t        _BaseRidgeCV.fit(self, X, Y, sample_weight=sample_weight)\n  1377\t        return self\n  1378\t\n  1379\t    @property\n  1380\t    def classes_(self):\n  1381\t        return self._label_binarizer.classes_\n",
      "perturbed": "1241\t    RidgeClassifier : Ridge classifier\n1240\t    Ridge : Ridge regression\n1243\t    \"\"\"\n1245\n1244\t    pass\n1247\tclass RidgeClassifierCV(LinearClassifierMixin, _BaseRidgeCV):\n1249\n1248\t    \"\"\"Ridge classifier with built-in cross-validation.\n1251\t    efficient Leave-One-Out cross-validation. Currentl...\n1252\t    n_samples case is handled efficiently.\n1253\n1254\t    Read more in the :ref:`User Guide <ridge_regression>`.\n1255\n1256\t    Parameters\n1258\t    alphas : numpy array of shape [n_alphas]\n1257\t    ----------\n1259\t        Array of alpha values to try.\n1260\t        Regulariz...\n1261\t        improves the conditioning of the problem and reduces the variance of\n1263\t        Alpha corresponds to ``C^-1`` in other linear models such as\n1264\t        LogisticRegression or LinearSVC.\n1265\n1266\t    fit_intercept : boolean\n1267\t        Whether to calculate the intercept for this model. If set\n1268\t        to false, no intercept will be used in calculations\n1269\t        (e.g. data is expected to be already centered).\n1270\n1273\t        If True, the regressors X will be normalized before regression by\n1271\t    normalize : boolean, optional, default False\n1278\n1275\t        If you wish to standardize, please use\n1279\t    scoring : string, ca...\n1281\t        a scorer callable object / function with sign...\n1280\t        A string (see model evaluation documentation) or\n1282\t        ``scorer(estimator, X, y)``.\n1283\n1284\t    cv : int, cross-validation generator or an iterable, optional\n1285\t        Determines the cross-validation splitting strategy.\n1286\t        Possible inputs ...\n1289\t        - integer, to specify the number of folds.\n1287\n1290\t        - An object to be used as a cross-validation generator.\n1291\t        - An iterable yielding train/test splits.\n1293\t        Refer :ref:`User Guide <cross_validation>` for the various\n1294\t        cross-validation strategies that can be used here.\n1297\t        Weights associated with classes in the form ``{class_label: weight}``.\n1295\n1298\t        If no...\n1302\t        as ``n_samples / (n_classes * np.bincount(y))``\n1299\n1303\n1305\t    ----------\n1304\t    Attributes\n1306\t    cv_values_ : array, shape = [n_samples, n_alphas] or \\\n1307\t    shape = [n_samples, n_responses, n_alphas], optional\n1309\t    `cv=None`). After `fit()` has been called, this attribute will contain \\\n1308\t        Cross-validation values for each alpha (if `store_cv_values=True` and\n1310\t    the mean squared errors (by default) or the values of the \\\n1312\n1311\t    `{loss,score}_func` function (if provided in the constructor).\n1313\t    coef_ : array, shape = [n_features] or [n_targets, n_features]\n1314\t        Weight vector(s).\n1317\t        Independent term in decision function...\n1318\t        ``fit_intercept = False``.\n1319\n1320\t    alpha_ : float\n1321\t        Estimated regularization parameter\n1323\t    See also\n1322\n1325\t    Ridge : Ridge regression\n1324\t    --------\n1326\t    RidgeClassifier : Ridge classifier\n1327\t    RidgeCV : Ridge regression with built-in cross validation\n1328\n1329\t    Notes\n1331\t    For multi-class classification, n_class classifiers are trained in\n1330\t    -----\n1332\t    a one-versus-all approach. Concretely, this is implemented by taking\n1333\t    advantage of the multi-variate response support in Ridge.\n1336\t                 normalize=False, scoring=None, cv=None, class_weight=None):\n1335\t    def __init__(self, alphas=(0.1, 1.0, 10.0), fit_intercept=True,\n1337\t        super(RidgeClassifierCV, self).__init__(\n1340\t        self.class_weight = class_weight\n1338\t            alphas=alphas, fit_intercept=fit_intercept, normalize=normalize,\n1341\n1343\t        \"\"\"Fit the ridge classifier.\n1342\t    def fit(self, X, y, sample_weight=None):\n1344\n1345\t        Parameters\n1347\t        X : array-like, shape (n_samples, n_features)\n1349\t            and n_features is the number of features.\n1348\t            Training vectors, where n_samples is the number of samples\n1350\n1351\t        y : array-like, shape (n_samples,)\n1352\t            Target values. Will be cast to X's dtype if necessary\n1354\t        sample_weight : float or numpy array of shape (n_samples,)\n1355\t            Sample weight.\n1358\t        -------\n1361\t        check_X_y(X, y, accept_sparse=['csr', 'csc', 'coo'],\n1360\t        \"\"\"\n1362\t                  multi_output=True)\n1363\n1364\t        self._label_binarizer = LabelBinarizer(pos_label=1, neg_label=-1)\n1366\t        if not self._label_binarizer.y_type_.startswith('multilabel'):\n1365\t        Y = self._label_binarizer.fit_transform(y)\n1367\t            y = column_or_1d(y, warn=True)\n1368\n1371\t                sample_weight = 1.\n1373\t            sample_weight = (sample_weight *\n1372\t            # modify the sample weights with the corresponding class weight\n1376\t        _BaseRidgeCV.fit(self, X, Y, sample_weight=sample_weight)\n1377\t        return self\n1379\t    @property\n1380\t    def classes_(self):",
      "applied_operators": [
        "text_perturbation"
      ]
    },
    {
      "action": "grep -n \"class _BaseRidgeCV\" -n sklearn/linear_model/ridge.py && nl -ba sklearn/linear_model/ridge.py | sed -n '1000,1248p'",
      "original": "1087:class _BaseRidgeCV(LinearModel):\n  1000\t                         multi_output=True, y_numeric=True)\n  1001\t        if sample_weight is not None and not isinstance(sample_weight, float):\n  1002\t            sample_weight = check_array(sample_weight, ensure_2d=False)\n  1003\t        n_samples, n_features = X.shape\n  1004\t\n  1005\t        X, y, X_offset, y_offset, X_scale = LinearModel._preprocess_data(\n  1006\t            X, y, self.fit_intercept, self.normalize, self.copy_X,\n  1007\t            sample_weight=sample_weight)\n  1008\t\n  1009\t        gcv_mode = self.gcv_mode\n  1010\t        with_sw = len(np.shape(sample_weight))\n  1011\t\n  1012\t        if gcv_mode is None or gcv_mode == 'auto':\n  1013\t            if sparse.issparse(X) or n_features > n_samples or with_sw:\n  1014\t                gcv_mode = 'eigen'\n  1015\t            else:\n  1016\t                gcv_mode = 'svd'\n  1017\t        elif gcv_mode == \"svd\" and with_sw:\n  1018\t            # FIXME non-uniform sample weights not yet supported\n  1019\t            warnings.warn(\"non-uniform sample weights unsupported for svd, \"\n  1020\t                          \"forcing usage of eigen\")\n  1021\t            gcv_mode = 'eigen'\n  1022\t\n  1023\t        if gcv_mode == 'eigen':\n  1024\t            _pre_compute = self._pre_compute\n  1025\t            _errors = self._errors\n  1026\t            _values = self._values\n  1027\t        elif gcv_mode == 'svd':\n  1028\t            # assert n_samples >= n_features\n  1029\t            _pre_compute = self._pre_compute_svd\n  1030\t            _errors = self._errors_svd\n  1031\t            _values = self._values_svd\n  1032\t        else:\n  1033\t            raise ValueError('bad gcv_mode \"%s\"' % gcv_mode)\n  1034\t\n  1035\t        if sample_weight is not None:\n  1036\t            X, y = _rescale_data(X, y, sample_weight)\n  1037\t\n  1038\t        centered_kernel = not sparse.issparse(X) and self.fit_intercept\n  1039\t\n  1040\t        v, Q, QT_y = _pre_compute(X, y, centered_kernel)\n  1041\t        n_y = 1 if len(y.shape) == 1 else y.shape[1]\n  1042\t        cv_values = np.zeros((n_samples * n_y, len(self.alphas)))\n  1043\t        C = []\n  1044\t\n  1045\t        scorer = check_scoring(self, scoring=self.scoring, allow_none=True)\n  1046\t        error = scorer is None\n  1047\t\n  1048\t        for i, alpha in enumerate(self.alphas):\n  1049\t            if error:\n  1050\t                out, c = _errors(alpha, y, v, Q, QT_y)\n  1051\t            else:\n  1052\t                out, c = _values(alpha, y, v, Q, QT_y)\n  1053\t            cv_values[:, i] = out.ravel()\n  1054\t            C.append(c)\n  1055\t\n  1056\t        if error:\n  1057\t            best = cv_values.mean(axis=0).argmin()\n  1058\t        else:\n  1059\t            # The scorer want an object that will make the predictions but\n  1060\t            # they are already computed efficiently by _RidgeGCV. This\n  1061\t            # identity_estimator will just return them\n  1062\t            def identity_estimator():\n  1063\t                pass\n  1064\t            identity_estimator.decision_function = lambda y_predict: y_predict\n  1065\t            identity_estimator.predict = lambda y_predict: y_predict\n  1066\t\n  1067\t            out = [scorer(identity_estimator, y.ravel(), cv_values[:, i])\n  1068\t                   for i in range(len(self.alphas))]\n  1069\t            best = np.argmax(out)\n  1070\t\n  1071\t        self.alpha_ = self.alphas[best]\n  1072\t        self.dual_coef_ = C[best]\n  1073\t        self.coef_ = safe_sparse_dot(self.dual_coef_.T, X)\n  1074\t\n  1075\t        self._set_intercept(X_offset, y_offset, X_scale)\n  1076\t\n  1077\t        if self.store_cv_values:\n  1078\t            if len(y.shape) == 1:\n  1079\t                cv_values_shape = n_samples, len(self.alphas)\n  1080\t            else:\n  1081\t                cv_values_shape = n_samples, n_y, len(self.alphas)\n  1082\t            self.cv_values_ = cv_values.reshape(cv_values_shape)\n  1083\t\n  1084\t        return self\n  1085\t\n  1086\t\n  1087\tclass _BaseRidgeCV(LinearModel):\n  1088\t    def __init__(self, alphas=(0.1, 1.0, 10.0),\n  1089\t                 fit_intercept=True, normalize=False, scoring=None,\n  1090\t                 cv=None, gcv_mode=None,\n  1091\t                 store_cv_values=False):\n  1092\t        self.alphas = alphas\n  1093\t        self.fit_intercept = fit_intercept\n  1094\t        self.normalize = normalize\n  1095\t        self.scoring = scoring\n  1096\t        self.cv = cv\n  1097\t        self.gcv_mode = gcv_mode\n  1098\t        self.store_cv_values = store_cv_values\n  1099\t\n  1100\t    def fit(self, X, y, sample_weight=None):\n  1101\t        \"\"\"Fit Ridge regression model\n  1102\t\n  1103\t        Parameters\n  1104\t        ----------\n  1105\t        X : array-like, shape = [n_samples, n_features]\n  1106\t            Training data\n  1107\t\n  1108\t        y : array-like, shape = [n_samples] or [n_samples, n_targets]\n  1109\t            Target values. Will be cast to X's dtype if necessary\n  1110\t\n  1111\t        sample_weight : float or array-like of shape [n_samples]\n  1112\t            Sample weight\n  1113\t\n  1114\t        Returns\n  1115\t        -------\n  1116\t        self : object\n  1117\t        \"\"\"\n  1118\t        if self.cv is None:\n  1119\t            estimator = _RidgeGCV(self.alphas,\n  1120\t                                  fit_intercept=self.fit_intercept,\n  1121\t                                  normalize=self.normalize,\n  1122\t                                  scoring=self.scoring,\n  1123\t                                  gcv_mode=self.gcv_mode,\n  1124\t                                  store_cv_values=self.store_cv_values)\n  1125\t            estimator.fit(X, y, sample_weight=sample_weight)\n  1126\t            self.alpha_ = estimator.alpha_\n  1127\t            if self.store_cv_values:\n  1128\t                self.cv_values_ = estimator.cv_values_\n  1129\t        else:\n  1130\t            if self.store_cv_values:\n  1131\t                raise ValueError(\"cv!=None and store_cv_values=True \"\n  1132\t                                 \" are incompatible\")\n  1133\t            parameters = {'alpha': self.alphas}\n  1134\t            gs = GridSearchCV(Ridge(fit_intercept=self.fit_intercept,\n  1135\t                                    normalize=self.normalize),\n  1136\t                              parameters, cv=self.cv, scoring=self.scoring)\n  1137\t            gs.fit(X, y, sample_weight=sample_weight)\n  1138\t            estimator = gs.best_estimator_\n  1139\t            self.alpha_ = gs.best_estimator_.alpha\n  1140\t\n  1141\t        self.coef_ = estimator.coef_\n  1142\t        self.intercept_ = estimator.intercept_\n  1143\t\n  1144\t        return self\n  1145\t\n  1146\t\n  1147\tclass RidgeCV(_BaseRidgeCV, RegressorMixin):\n  1148\t    \"\"\"Ridge regression with built-in cross-validation.\n  1149\t\n  1150\t    By default, it performs Generalized Cross-Validation, which is a form of\n  1151\t    efficient Leave-One-Out cross-validation.\n  1152\t\n  1153\t    Read more in the :ref:`User Guide <ridge_regression>`.\n  1154\t\n  1155\t    Parameters\n  1156\t    ----------\n  1157\t    alphas : numpy array of shape [n_alphas]\n  1158\t        Array of alpha values to try.\n  1159\t        Regularization strength; must be a positive float. Regularization\n  1160\t        improves the conditioning of the problem and reduces the variance of\n  1161\t        the estimates. Larger values specify stronger regularization.\n  1162\t        Alpha corresponds to ``C^-1`` in other linear models such as\n  1163\t        LogisticRegression or LinearSVC.\n  1164\t\n  1165\t    fit_intercept : boolean\n  1166\t        Whether to calculate the intercept for this model. If set\n  1167\t        to false, no intercept will be used in calculations\n  1168\t        (e.g. data is expected to be already centered).\n  1169\t\n  1170\t    normalize : boolean, optional, default False\n  1171\t        This parameter is ignored when ``fit_intercept`` is set to False.\n  1172\t        If True, the regressors X will be normalized before regression by\n  1173\t        subtracting the mean and dividing by the l2-norm.\n  1174\t        If you wish to standardize, please use\n  1175\t        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\n  1176\t        on an estimator with ``normalize=False``.\n  1177\t\n  1178\t    scoring : string, callable or None, optional, default: None\n  1179\t        A string (see model evaluation documentation) or\n  1180\t        a scorer callable object / function with signature\n  1181\t        ``scorer(estimator, X, y)``.\n  1182\t\n  1183\t    cv : int, cross-validation generator or an iterable, optional\n  1184\t        Determines the cross-validation splitting strategy.\n  1185\t        Possible inputs for cv are:\n  1186\t\n  1187\t        - None, to use the efficient Leave-One-Out cross-validation\n  1188\t        - integer, to specify the number of folds.\n  1189\t        - An object to be used as a cross-validation generator.\n  1190\t        - An iterable yielding train/test splits.\n  1191\t\n  1192\t        For integer/None inputs, if ``y`` is binary or multiclass,\n  1193\t        :class:`sklearn.model_selection.StratifiedKFold` is used, else,\n  1194\t        :class:`sklearn.model_selection.KFold` is used.\n  1195\t\n  1196\t        Refer :ref:`User Guide <cross_validation>` for the various\n  1197\t        cross-validation strategies that can be used here.\n  1198\t\n  1199\t    gcv_mode : {None, 'auto', 'svd', eigen'}, optional\n  1200\t        Flag indicating which strategy to use when performing\n  1201\t        Generalized Cross-Validation. Options are::\n  1202\t\n  1203\t            'auto' : use svd if n_samples > n_features or when X is a sparse\n  1204\t                     matrix, otherwise use eigen\n  1205\t            'svd' : force computation via singular value decomposition of X\n  1206\t                    (does not work for sparse matrices)\n  1207\t            'eigen' : force computation via eigendecomposition of X^T X\n  1208\t\n  1209\t        The 'auto' mode is the default and is intended to pick the cheaper\n  1210\t        option of the two depending upon the shape and format of the training\n  1211\t        data.\n  1212\t\n  1213\t    store_cv_values : boolean, default=False\n  1214\t        Flag indicating if the cross-validation values corresponding to\n  1215\t        each alpha should be stored in the `cv_values_` attribute (see\n  1216\t        below). This flag is only compatible with `cv=None` (i.e. using\n  1217\t        Generalized Cross-Validation).\n  1218\t\n  1219\t    Attributes\n  1220\t    ----------\n  1221\t    cv_values_ : array, shape = [n_samples, n_alphas] or \\\n  1222\t        shape = [n_samples, n_targets, n_alphas], optional\n  1223\t        Cross-validation values for each alpha (if `store_cv_values=True` and \\\n  1224\t        `cv=None`). After `fit()` has been called, this attribute will \\\n  1225\t        contain the mean squared errors (by default) or the values of the \\\n  1226\t        `{loss,score}_func` function (if provided in the constructor).\n  1227\t\n  1228\t    coef_ : array, shape = [n_features] or [n_targets, n_features]\n  1229\t        Weight vector(s).\n  1230\t\n  1231\t    intercept_ : float | array, shape = (n_targets,)\n  1232\t        Independent term in decision function. Set to 0.0 if\n  1233\t        ``fit_intercept = False``.\n  1234\t\n  1235\t    alpha_ : float\n  1236\t        Estimated regularization parameter.\n  1237\t\n  1238\t    See also\n  1239\t    --------\n  1240\t    Ridge : Ridge regression\n  1241\t    RidgeClassifier : Ridge classifier\n  1242\t    RidgeClassifierCV : Ridge classifier with built-in cross validation\n  1243\t    \"\"\"\n  1244\t    pass\n  1245\t\n  1246\t\n  1247\tclass RidgeClassifierCV(LinearClassifierMixin, _BaseRidgeCV):\n  1248\t    \"\"\"Ridge classifier with built-in cross-validation.\n",
      "perturbed": "1000\t                         multi_output=True, y_numeric=True)\n1003\t        n_samples, n_features = X.shape\n1004\n1005\t        X, y, X_offset, y_offset, X_scale = LinearModel._preprocess_data(\n1008\n1007\t            sample_weight=sample_weight)\n1009\t        gcv_mode = self.gcv_mode\n1010\t        with_sw = len(np.shape(sample_weight))\n1012\t        if gcv_mode is None or gcv_mode == 'auto':\n1011\n1013\t            if sparse.issparse(X) or n_features > n_samples or with_sw:\n1015\t            else:\n1016\t                gcv_mode = 'svd'\n1017\t        elif gcv_mode == \"svd\" and with_sw:\n1019\t            warnings.warn(\"non-uniform sample weights unsupported for svd, \"\n1020\t                          \"forcing usage of eigen\")\n1021\t            gcv_mode = 'eigen'\n1023\t        if gcv_mode == 'eigen':\n1022\n1024\t            _pre_compute = self._pre_compute\n1025\t            _errors = self._errors\n1026\t            _values = self._values\n1028\t            # assert n_samples >= n_features\n1029\t            _pre_compute = self._pre_compute_svd\n1030\t            _errors = self._errors_svd\n1031\t            _values = self._values_svd\n1032\t        else:\n1033\t            raise ValueError('bad gcv_mode \"%s\"' % gcv_mode)\n1034\n1035\t        if sample_weight is not ...\n1037\n1038\t        centered_kernel = not sparse.issparse(X) and self.fit_intercept\n1040\t        v, Q, QT_y = _pre_compute(X, y, centered_kernel)\n1039\n1041\t        n_y = 1 if len(y.shape) == 1 else y.shape[1]\n1042\t        cv_values = np.zeros((n_samples * n_y, len(self.alphas)))\n1044\n1045\t        scorer = check_scoring(self, scoring=self.scoring, allow_none=True)\n1046\t        error = scorer is None\n1050\t                out, c = _errors(alpha, y, v, Q, ...\n1047\n1051\t            else:\n1052\t                out, c = _values(alpha, y, v, Q, QT_y)\n1053\t            cv_values[:, i] = out.ravel()\n1055\n1056\t        if error:\n1057\t            best = cv_values.mean(axis=0).argmin()\n1058\t        else:\n1059\t            # The scorer want an object that will make the predictions but\n1061\t            # identity_estimator will...\n1060\t            # they are already computed effi...\n1063\t                pass\n1064\t            identity_estimator.decision_function = lambda y_predict: y_predict\n1068\t                   for i in range(len(s...\n1065\t            identity_estimator.predict = lambda y_predict: y_p...\n1070\n1071\t        self.alpha_ = self.alphas[best]\n1072\t        self.dual_coef_ = C[best]\n1074\n1073\t        self.coef_ = safe_sparse_dot(...\n1075\t        self._set_intercept(X_offset, y_offset, X_scale)\n1077\t        if self.store_cv_values:\n1078\t            if len(y.shape) == 1:\n1083\n1081\t                cv_values_shape = n_samples, n_y, len(self.alphas)\n1084\t        return self\n1085\n1086\n1088\t    def __init__(self, alphas=(0.1, 1.0, 10.0),\n1087\tclass _BaseRidgeCV(LinearModel):\n1090\t                 cv=None, gcv_mode=None,\n1091\t                 store_cv_values=False):\n1093\t        self.fit_intercept = fit_intercept\n1092\t        self.alphas = alphas\n1094\t        self.normalize = normalize\n1095\t        self.scoring = scoring\n1097\t        self.gcv_mode = gcv_mode\n1096\t        self.cv = cv\n1099\n1098\t        self.store_cv_values = store_cv_values\n1100\t    def fit(self, X, y, sample_weight=None):\n1101\t        \"\"\"Fit Ridge regress...\n1104\t        ----------\n1103\t        Parameters\n1105\t        X : array-like, shape = [n_samples, n_features]\n1107\n1106\t            Training data\n1108\t        y : array-like, shape = [n_samples] or [n_samples, n_targets]\n1109\t            Target values. Will be cast to X's dtype if necessary\n1110\n1111\t        sample_weight : float or array-like of shape [n_samples]\n1112\t            Sample weight\n1113\n1116\t        self : object\n1117\t        \"\"\"\n1118\t        if self.cv is None:\n1122\t                                  scoring=self.scoring,\n1126\t            self.alpha_ = estimator.alpha_\n1123\t                                  gcv_mode=self.gcv_...\n1127\t            if self.store_cv_values:\n1128\t                self.cv_values_ = estimator.cv_values_\n1130\t            if self.store_cv_values:\n1131\t                raise ValueError(\"cv!=None and store_cv_values=True \"\n1132\t                                 \" are incompati...\n1133\t            parameters = {'alpha': self.alphas}\n1135\t                                    normalize=self.normalize),\n1137\t            gs.fit(X, y, sample_weight=sample_weight)\n1139\t            self.alpha_ = gs.best_estimator_.alpha\n1140\n1142\t        self.intercept_ = estimator.intercept_\n1141\t        self.coef_ = estimator.coef_\n1143\n1144\t        return self\n1145\n1146\n1147\tclass RidgeCV(_BaseRidgeCV, RegressorMixin):\n1148\t    \"\"\"Ridge regression with built-in cross-validation.\n1149\n1152\n1151\t    efficient Leave-One-Out cross-...\n1153\t    Read more in the :ref:`User Guide <ridge_regression>`.\n1155\t    Parameters\n1154\n1156\t    ----------\n1157\t    alphas : numpy array of shape [n_alphas]\n1158\t        Array of alpha values to try.\n1163\t        LogisticRegression or LinearSVC.\n1162\t        Alpha corresponds to ``C...\n1165\t    fit_intercept : boolean\n1164\n1167\t        to false, no intercept will be used in calculations\n1171\t        This parameter is ignored when ``fit_intercept`` is set to False.\n1169\n1172\t        If True, the regressors X will be normalized before regression by\n1176\t        on an estimator with ``normalize=False``.\n1175\t        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\n1177\n1178\t    scoring : string, callable or None, optional, default: None\n1181\t        ``scorer(estimator, X, y)``.\n1184\t        Determines the cross-validation splitting strategy.\n1183\t    cv : int, cross-validation generator or an iterable, optional\n1187\t        - None, to use the efficient Leave-One-Out cross-validation\n1186\n1188\t        - integer, to specify the number of folds.\n1189\t        - An object to be used as a cross-validation generator.\n1190\t        - An iterable yielding train/test splits.\n1191\n1193\t        :class:`sklearn.model_selection.StratifiedKFold` is used, else,\n1194\t        :class:`sklearn.model_sel...\n1195\n1196\t        Refer :ref:`User Guide <cross_validation>` for the various\n1199\t    gcv_mode : {None, 'auto', 'svd', eigen'}, optional\n1198\n1200\t        Flag indicating which strategy to use when performing\n1201\t        Generalized Cross-Validation. Options are::\n1203\t            'auto' : use svd if n_samples > n_features or when X is a sparse\n1205\t            'svd' : force computation via singular value decom...\n1206\t                    (does not work for sparse matrices)\n1207\t            'eigen' : force computation via eigendecomposition of X^T X\n1208\n1209\t        The 'auto' mode is the default and is intended to pick the cheaper\n1212\n1213\t    store_cv_values : boolean, default=False\n1217\t        Generalized Cross-Validation).\n1216\t        below). This flag is only compatible with `cv=None` (i.e. using\n1218\n1219\t    Attributes\n1221\t    cv_values_ : array, shape = [n_samples, n_alp...\n1220\t    ----------\n1222\t        shape = [n_samples, n_targets, n_alphas], optional\n1226\t        `{loss...\n1228\t    coef_ : array, shape = [n_features] or [n_targets, n_features]\n1227\n1230\n1229\t        Weight vector(s).\n1231\t    intercept_ : float | array, shape = (n_targets,)\n1233\t        ``fit_intercept = False``.\n1234\n1235\t    alpha_ : float\n1236\t        Estimated regularization parameter.\n1238\t    See also\n1239\t    --------\n1244\t    pass\n1240\t    Ridge : Ridge regression\n1245\n1246\n1248\t    \"\"\"Ridge classifier with built-in cr...",
      "applied_operators": [
        "text_perturbation"
      ]
    },
    {
      "action": "nl -ba sklearn/linear_model/ridge.py | sed -n '1328,1348p'",
      "original": "  1328\t\n  1329\t    Notes\n  1330\t    -----\n  1331\t    For multi-class classification, n_class classifiers are trained in\n  1332\t    a one-versus-all approach. Concretely, this is implemented by taking\n  1333\t    advantage of the multi-variate response support in Ridge.\n  1334\t    \"\"\"\n  1335\t    def __init__(self, alphas=(0.1, 1.0, 10.0), fit_intercept=True,\n  1336\t                 normalize=False, scoring=None, cv=None, class_weight=None):\n  1337\t        super(RidgeClassifierCV, self).__init__(\n  1338\t            alphas=alphas, fit_intercept=fit_intercept, normalize=normalize,\n  1339\t            scoring=scoring, cv=cv)\n  1340\t        self.class_weight = class_weight\n  1341\t\n  1342\t    def fit(self, X, y, sample_weight=None):\n  1343\t        \"\"\"Fit the ridge classifier.\n  1344\t\n  1345\t        Parameters\n  1346\t        ----------\n  1347\t        X : array-like, shape (n_samples, n_features)\n  1348\t            Training vectors, where n_samples is the number of samples\n",
      "perturbed": "1328\n1330\t    -----\n[info] processing...\n1331\t    For multi-class classification, n_class classifiers are trained in\n1332\t    a one-versus-all approach. Concretely, this is implemented by taking\n1333\t    advantage of the multi-variate response support in Ridge.\n1334\t    \"\"\"\n1335\t    def __init__(self, alphas=(0.1, 1.0, 10.0), fit_intercept=True,\n1336\t                 normalize=False, scoring=None, cv=None, class_weight=None):\n1338\t            alphas=alphas, fit_intercept=fit_intercept, normalize=normalize,\n1337\t        super(RidgeClassifierCV, self).__init__(\n[warning] unexpected output\n1341\n1340\t        self.class_weight = cla...\n1342\t    def fit(self, X, y, sample_weight=None):\n1344\n1348\t            Training vectors, where n_samples is the number of samples\ninfo: continuing execution",
      "applied_operators": [
        "text_perturbation"
      ]
    },
    {
      "action": "python3 - <<'PY'\nimport io,sys,re\np = \"sklearn/linear_model/ridge.py\"\ntext = open(p, \"r\", encoding=\"utf-8\").read()\n\n# Replace the RidgeClassifierCV __init__ signature to include store_cv_values=False\nold_sig = re.compile(\n    r\"(def __init__\\(\\s*self,\\s*alphas=\\(0\\.1, 1\\.0, 10\\.0\\),\\s*fit_intercept=True,\\s*\\n\\s*normalize=False,\\s*scoring=None,\\s*cv=None,\\s*class_weight=None\\):)\"\n)\nif old_sig.search(text):\n    text = old_sig.sub(\n        \"def __init__(self, alphas=(0.1, 1.0, 10.0), fit_intercept=True,\\n                 normalize=False, scoring=None, cv=None, store_cv_values=False, class_weight=None):\",\n        text, count=1\n    )\nelse:\n    # Fallback: more permissive replacement matching the typical signature line\n    text = re.sub(r\"(def __init__\\(\\s*self,[^\\)]*cv=None,)\\s*class_weight=None\\):\",\n                  r\"\\1 store_cv_values=False, class_weight=None):\", text, count=1)\n\n# Insert store_cv_values into the super(...) call argument list for RidgeClassifierCV\ntext = re.sub(\n    r\"(super\\(RidgeClassifierCV,\\s*self\\)\\.__init__\\(\\s*\\n\\s*alphas=alphas,\\s*fit_intercept=fit_intercept,\\s*normalize=normalize,)\",\n    r\"\\1 store_cv_values=store_cv_values,\",\n    text,\n    count=1\n)\n\nopen(p, \"w\", encoding=\"utf-8\").write(text)\nprint(\"patched\", p)\nPY",
      "original": "patched sklearn/linear_model/ridge.py\n",
      "perturbed": "patched sklearn/linear_model/ridge.py",
      "applied_operators": [
        "text_perturbation"
      ]
    },
    {
      "action": "python3 - <<'PY'\nimport numpy as np\nfrom sklearn import linear_model as lm\nX = np.random.RandomState(0).randn(20, 5)\ny = np.random.RandomState(1).randint(0, 2, size=20)\nclf = lm.RidgeClassifierCV(alphas=np.array([0.1, 1.0]), normalize=True, store_cv_values=True)\nclf.fit(X, y)\nprint(\"store_cv_values attribute:\", getattr(clf, \"store_cv_values\", None))\nprint(\"has cv_values_:\", hasattr(clf, \"cv_values_\"))\nprint(\"cv_values_ shape:\", None if not hasattr(clf, \"cv_values_\") else np.shape(clf.cv_values_))\nPY",
      "original": "store_cv_values attribute: True\nhas cv_values_: True\ncv_values_ shape: (20, 1, 2)\n",
      "perturbed": "store_cv_values attribute: True",
      "applied_operators": [
        "text_perturbation"
      ]
    },
    {
      "action": "pytest -q sklearn/linear_model/tests/test_ridge.py::test_ridge_classifier_cv -q",
      "original": "\nERROR: not found: /testbed/sklearn/linear_model/tests/test_ridge.py::test_ridge_classifier_cv\n(no name '/testbed/sklearn/linear_model/tests/test_ridge.py::test_ridge_classifier_cv' in any of [<DoctestModule sklearn/linear_model/tests/test_ridge.py>, <Module sklearn/linear_model/tests/test_ridge.py>])\n\n",
      "perturbed": "(no name '/testbed/sklearn/linear_model/tests/test_ridge.py::test_ridge_classifier_cv' in any of [<DoctestModule sklearn/linear_model/tests/test_ridge.py>, <Module sklearn/linear_model/tests/test_ridge.py>])\nERROR: not found: /testbed/sklearn/linear_model/tests/test_ridge.py::test_ridge_classifier_cv",
      "applied_operators": [
        "text_perturbation"
      ]
    },
    {
      "action": "pytest -q sklearn/linear_model/tests/test_ridge.py",
      "original": "..........x..................                                            [100%]\n28 passed, 1 xfailed, 22 warnings in 0.62s\n",
      "perturbed": "..........x..................                                            [100%]\n28 passed, 1 xfailed, 22 warnings in 0.62s",
      "applied_operators": [
        "text_perturbation"
      ]
    }
  ]
}